[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julia for Statistics",
    "section": "",
    "text": "Julia For Statistics",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "TP-PCA.html",
    "href": "TP-PCA.html",
    "title": "Principal component analysis (PCA)",
    "section": "",
    "text": "Principal component analysis (PCA) reduces the number of dimensions in large datasets to principal components that retain most of the original information. It does this by transforming potentially correlated variables into a smaller set of uncorrelated variables, called principal components.\nLet \\(X\\) the matrix \\((n,p)\\) of data, we note \\(X_c\\) = the centered matrix. Then the empirical variances, covariances matrix is \\(C = \\frac{1}{n}X_c^TX_c\\). We note \\(\\Lambda\\) the vector of the eigen value (in decrease order) of the matrix \\(C\\) and \\(U\\) the \\((p,p)\\) orthogonal matrix of the eigen vectors : \\[C=U\\texttt{diag}(\\Lambda) U^T.\\] Then We have\n\nThe coordinates of the \\(n\\) observations in the new basis of the eigen vectors \\((\\vec{u}_1,\\ldots,\\vec{u}_p)\\) are \\[\\Psi = X_cU\\]\nThe The coordinates of the \\(p\\) variables in the new basis of the eigen vectors \\((\\vec{v}_1,\\ldots,\\vec{v}_p)\\) are \\[\\Phi = \\sqrt{n}U\\texttt{diag}(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda}_p)\\]\nThe total inertia (variance) is \\[I = \\texttt{trace}(C)=\\sum_{i=1,p}\\lambda_i\\]\nThe variance of the variable \\(v_i\\) is \\(\\lambda_i\\)"
  },
  {
    "objectID": "TP-PCA.html#principal-component-analysis-pca",
    "href": "TP-PCA.html#principal-component-analysis-pca",
    "title": "Principal component analysis (PCA)",
    "section": "",
    "text": "Code\nusing Plots\nusing Statistics, LinearAlgebra\nusing RDatasets, DataFrames\niris = RDatasets.dataset(\"datasets\", \"iris\")  # Iris Datas\nNames = names(iris)\nX = Matrix(iris[:,1:3])\nn,p = size(X)\n#\np1 = scatter(X[:,1],X[:,2],X[:,3], c=[:blue :red :green], group=iris.Species,xlabel = Names[1],ylabel=Names[2],zlabel=Names[3])\np2 = scatter(X[:,1],X[:,2], c=[:blue :red :green], group=iris.Species,xlabel = Names[1],ylabel=Names[2])\np3 = scatter(X[:,1],X[:,3], c=[:blue :red :green], group=iris.Species,xlabel = Names[1],ylabel=Names[3])\nplot(p1,p2,p3)"
  },
  {
    "objectID": "TP-PCA.html#pca-on-iris-data",
    "href": "TP-PCA.html#pca-on-iris-data",
    "title": "Principal component analysis (PCA)",
    "section": "PCA on Iris Data",
    "text": "PCA on Iris Data\n\nMy PCA function\n\n\nCode\nusing LinearAlgebra, Statistics\nfunction my_PCA(X::Matrix{&lt;:Real})\n     n,p = size(X)\n     # Calculation of centered data\n     xbar = mean(X,dims=1)\n     Xc = X - ones(n,1)*xbar\n     covMat = (1/n)*Xc'*Xc\n     Λ, U = eigen(covMat)\n     eigOrder = sortperm(Λ, rev = true) # for abtaining increase order of eigen values\n     Λ = Λ[eigOrder]\n     U = U[:,eigOrder]\n     Ψ = Xc*U\n     Φ = U*sqrt.(n*diagm(Λ))\n     return Λ, U, Ψ, Φ\nend\n\n\nmy_PCA (generic function with 1 method)\n\n\n\n\nCode\nΛ, U, Ψ, Φ = my_PCA(X)\nprintln(\"lambda = \", Λ)\nprintln(\"U = \")\ndisplay(U)\nprintln(\"Ψ = \", Ψ)\nprintln(\"Φ = \")\ndisplay(Φ)\n\n\nlambda = [3.66651232367719, 0.23976809097032858, 0.059057363130254135]\nU = \n\n\n3×3 Matrix{Float64}:\n  0.389833   0.639223   0.66289\n -0.091008   0.743059  -0.663009\n  0.916377  -0.198135  -0.347844\n\n\nΨ = [-2.4908801764591226 0.3209733639915051 0.033974525061693565; -2.523342855807716 -0.17840062221707287 0.2329011355180754; -2.711148878738478 -0.13782005772483072 0.0025055723307144838; -2.5577559512662997 -0.3156752262309348 -0.06705123062327753; -2.538964320653582 0.3313569025645965 -0.09861543384842726; -2.1354201492905385 0.7505233495888126 -0.13671519038321095; -2.6766960896825687 -0.0729441403042631 -0.23116967382323433; -2.429124983528788 0.1629316828314618 -0.0007979232594163794; -2.709158769031162 -0.5723181265128411 -0.032243063366852516; -2.440805922557347 -0.12390824336959572 0.13181584827424744; -2.300494015791276 0.6415385918938586 0.06545538410444443; -2.4154539347929123 0.015273540244509057 -0.1681603304906478; -2.5623261987107004 -0.24232294983751945 0.16661210921890018; -3.0321561168511524 -0.5024941259015702 -0.06047995837832927; -2.4467762516792964 1.179585962994435 0.23606175536695004; -2.2472495961226486 1.3534466381099577 -0.19978406527465462; -2.5019710874579135 0.8297772989730531 0.0024222281483154027; -2.4908801764591226 0.3209733639915051 0.033974525061693565; -2.009369319290111 0.8679844662566139 0.12845282112525974; -2.4265448457917036 0.5240774752260565 -0.19971262740402648; -2.0899161428331636 0.37899401862112664 0.19478947267151997; -2.417444044500229 0.44977160903251956 -0.13341169479308054; -3.061448630432894 0.15492154146705187 -0.22463412051360016; -2.1977653702506412 0.11292116956625024 0.06222332638494126; -2.140540731167381 -0.04416692179367128 -0.2725133943892927; -2.301084043821044 -0.15410526928874713 0.2296214525514866; -2.337487248986944 0.14311819548540167 -0.03558227789229805; -2.3602590990142946 0.3650822042658916 0.0654791967279871; -2.4427960322646634 0.31058982541841446 0.16656448397181506; -2.4362356751129464 -0.19726051976301107 -0.10184749156793037; -2.3881515309184875 -0.20764405833610244 0.030742467342190567; -2.273191611916851 0.4186209933132468 0.26435818193728317; -2.414863906763144 0.8109174014271145 -0.3323263989376897; -2.3986524138875107 1.0968037378280506 -0.16497589801823026; -2.440805922557347 -0.12390824336959572 0.13181584827424744; -2.6858365845713696 0.07376041248256804 0.23615700586112073; -2.4265845393890295 0.5964761618193503 0.3339149848912747; -2.5779476635565657 0.26743457494415046 -0.1649044601476018; -2.80989730486448 -0.47819877297324387 -0.06375964134491728; -2.390141640625804 0.22685401045190773 0.06549110303975816; -2.62150125390395 0.2768645237171193 0.0024698533954005284; -2.7072083529211715 -0.9344175087075585 0.46663591323088066; -2.8280989074474303 -0.3295870405861693 -0.1963615065668098; -2.346588050278419 0.21742406167893893 -0.10188321050324432; -2.0599939076243285 0.444823525841816 -0.338850045935553; -2.5623261987107004 -0.24232294983751945 0.16661210921890018; -2.33490711124986 0.5042639878799964 -0.23449698203690814; -2.6584944870996186 -0.22155587269133736 -0.09856780860134214; -2.3394773586942605 0.5776162642734122 -0.0008336421947307788; -2.5116619167791567 0.10843930398398466 0.10028736398441145; 1.3011509824528487 0.6587349077793877 0.34448511969376; 0.8839754559512559 0.314827916748831 0.016319671164474612; 1.4545439099250275 0.48087973927328415 0.2749283167397684; 0.15684390873845386 -0.8301583908467197 0.19034860113482494; 1.0509997385619834 0.061713292249067776 0.3130280732745526; 0.6474952607962666 -0.4298518413684417 -0.1824997824859644; 1.0191667808404845 0.2855844806298011 -0.18583899701140985; -0.727621091763833 -1.0006920789534384 -0.030196006840998543; 1.0808822801734925 0.199941486063051 0.3130161669627811; -0.08814705967824242 -0.7048884215878489 -0.23893785357360275; -0.46895907461126135 -1.2736201907992615 0.23172804063619734; 0.4323471403937537 -0.0939549917022943 -0.07817053121086225; 0.3608614245448496 -0.5848526189380256 0.588094665241645; 0.977603300200416 -0.13948363938524025 -0.05321331916597499; -0.22532849427478727 -0.2411469166808099 -0.0020305497001511276; 0.9183885514098398 0.45210252076269225 0.3162720373058267; 0.5903103153103323 -0.3451624366018137 -0.3813906740070321; 0.3290284668233499 -0.3609814305572921 0.08922759495568283; 0.8970167830600373 -0.5560754004274336 0.5467509446755867; 0.08598791451664399 -0.5978108434931391 0.1588201168449887; 0.9639719450618666 -0.06422418339158041 -0.41947852423004445; 0.34523995969898386 -0.07509509415635661 0.2565780958751428; 1.275248660255972 -0.34848942361061674 0.2749997546103962; 0.986704101491891 -0.21378950557877752 0.013087613444971283; 0.728002390741993 0.1315372928603394 0.28479117826307665; 0.8885060097983305 0.3138743269487084 0.3162839436175979; 1.3512252363546238 0.21385330041828593 0.44232644290631373; 1.4773157599523776 0.25891573049279426 0.17386684211948328; 0.7553444882137441 -0.16377899231356594 -0.049933636199386254; -0.25068048203922194 -0.38032870029491467 0.29794562906474414; -0.03553236163670899 -0.7162255499610621 0.193616377789642; -0.12717009617855252 -0.696412062615002 0.2284007324225235; 0.14575299773966244 -0.32135445586517186 0.15879630422144603; 1.3233724980477568 -0.4312716487770008 -0.12603789877478375; 0.5123436295043643 -0.4730070918427056 -0.5139687266053813; 0.7098404817563693 0.2077503386541201 -0.38143829925411715; 1.1933017550353713 0.39266205872451193 0.21191897340718185; 0.8352615901297028 -0.3980337192673907 0.5815233929966969; 0.2237593771429566 -0.26590848721757315 -0.24225325547550539; 0.13864230615550388 -0.6815466584596451 0.057746735912932425; 0.49609244303140465 -0.6864947416503484 -0.14769161522954047; 0.8768647643670967 -0.045364285845642734 -0.08472989714403942; 0.24649153357298134 -0.4154738094047693 0.1903128821995106; -0.6795369475693741 -1.0110756175265296 0.10239395206912225; 0.34269951555922584 -0.5086395731442449 -0.07813481227554879; 0.35438045458778533 -0.22179964694318682 -0.21074858380921205; 0.3634812558792603 -0.2961055131367241 -0.1444476511982658; 0.6500357049360247 0.0036926376194468787 0.1522131256647269; -0.9336684108748714 -0.7391010954808289 0.14043417704504924; 0.2809443226288909 -0.35059789198420116 -0.04336236395443773; 2.2104573298844543 0.028009145131019472 -0.6380356072388711; 1.2454058122417884 -0.5591163040178934 -0.25861595137313353; 2.457988742440909 0.33628365486003753 0.1259637556202475; 1.8803095968829786 -0.1899603702588887 -0.23369445826355972; 2.13245095048116 -0.027436823516579185 -0.23698604754191951; 3.294369598748736 0.517200881539848 0.21391840468595036; 0.3629309214468183 -1.1641480609126227 -0.5139091950465244; 2.911607167705727 0.31056849452315244 0.18570532229801706; 2.2559216427445032 -0.2711214992433727 0.22709666811116117; 2.625642746678731 0.8064142049495874 -0.2751215230120176; 1.4727852061053028 0.2598693202929162 -0.1260974303336405; 1.6625813387433814 -0.21520931298733606 0.0695494971561525; 1.9744877755645809 0.22377062138293957 0.06623409525424975; 1.1329863373799107 -0.7518368766793538 -0.15751875781753416; 1.2363050109503135 -0.4848104378243564 -0.3249168839840795; 1.6170773322860066 0.15632001798035 -0.2619551658985784; 1.8575377468556284 0.03200363852160115 -0.13263298364327467; 3.352184265861765 1.1557566513625315 -0.28498438453532554; 3.6446693504431527 0.22445928234796506 0.4410580975302652; 1.2772387699632881 -0.7829874923986269 0.24025111891282863; 2.1785449849683034 0.39667770669834046 -0.06964745293423091; 0.975062856060658 -0.5730281183731288 -0.38792622731666626; 3.4431922787765146 0.41269798942715963 0.3780249415741364; 1.2570470576730222 -0.19987769122354218 0.14239788938850365; 2.09147749787086 0.3431389176509849 -0.26852643814352667; 2.570408217302787 0.5290042275214987 0.02486656206464855; 1.117325178936719 -0.16968066530439102 0.04459228511126493; 1.1517779679926288 -0.10480474788382324 -0.18908296104268457; 1.9283937410774379 -0.20034390883197944 -0.10110449935343829; 2.405334350802049 0.42001946982654437 0.22703713655230442; 2.776415842816499 0.339811930642182 0.387863990473902; 3.155237748042202 1.3430417686416045 -0.04805326803833093; 1.9283937410774379 -0.20034390883197944 -0.10110449935343829; 1.4312217254652344 -0.16519879972212534 0.006528247511794738; 1.8296453149514351 -0.5407226240803927 -0.1673697130290711; 2.8751642689425014 0.680190645890595 0.4541292041495339; 1.8348055904256038 0.18156896070879733 -0.5651991213182906; 1.8094536026611694 0.04238717709469242 -0.2652229425533955; 1.0211568905478006 -0.148913588158209 -0.22058763270897735; 1.9127325826342467 0.3818123025429835 0.10100654357536021; 2.0180413659119654 0.2143406726099709 -0.10114021828875269; 1.6378193790087145 0.441252764581164 0.20535960747400536; 1.2454058122417884 -0.5591163040178934 -0.25861595137313353; 2.3228371111490067 0.2931284043857736 -0.20550518849916946; 2.09147749787086 0.3431389176509849 -0.26852643814352667; 1.6605912290360654 0.219288755800674 0.10429813285371994; 1.3668863947978156 -0.3683029109566768 0.24021539997751468; 1.5826245432300972 0.09144410055978149 -0.028279919744629813; 1.6125467784389327 0.15727360778047147 -0.5619194383517022; 1.2570867512703479 -0.2722763778168353 -0.3912297229067968]\nΦ = \n\n\n3×3 Matrix{Float64}:\n  9.14221   3.83349   1.97299\n -2.13428   4.4562   -1.97334\n 21.4905   -1.18823  -1.0353\n\n\n\n\nCode\nusing MultivariateStats\n\nmodel = fit(PCA, X', maxoutdim=3, pratio = 0.99)\nPsi1 = projection(model)\n\n\n3×3 Matrix{Float64}:\n  0.389833   0.639223   0.66289\n -0.091008   0.743059  -0.663009\n  0.916377  -0.198135  -0.347844"
  },
  {
    "objectID": "PCA.html",
    "href": "PCA.html",
    "title": "Principal component analysis (PCA)",
    "section": "",
    "text": "Principal component analysis (PCA) reduces the number of dimensions in large datasets to principal components that retain most of the original information. It does this by transforming potentially correlated variables into a smaller set of uncorrelated variables, called principal components.\nLet \\(X\\) the matrix \\((n,p)\\) of data, we note \\(X_c\\) = the centered matrix. Then the empirical variances, covariances matrix is \\(C = \\frac{1}{n}X_c^TX_c\\). We note \\(\\Lambda\\) the vector of the eigen value (in decrease order) of the matrix \\(C\\) and \\(U\\) the \\((p,p)\\) orthogonal matrix of the eigen vectors : \\[C=U\\texttt{diag}(\\Lambda) U^T.\\] Then We have\n\nThe coordinates of the \\(n\\) observations in the new basis of the eigen vectors \\((\\vec{u}_1,\\ldots,\\vec{u}_p)\\) are \\[\\Psi = X_cU\\]\nThe The coordinates of the \\(p\\) variables in the new basis of the eigen vectors \\((\\vec{v}_1,\\ldots,\\vec{v}_p)\\) are \\[\\Phi = \\sqrt{n}U\\texttt{diag}(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda}_p)\\]\nThe total inertia (variance) is \\[I = \\texttt{trace}(C)=\\sum_{i=1,p}\\lambda_i\\]\nThe variance of the variable \\(v_i\\) is \\(\\lambda_i\\)",
    "crumbs": [
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "PCA.html#principal-component-analysis-pca",
    "href": "PCA.html#principal-component-analysis-pca",
    "title": "Principal component analysis (PCA)",
    "section": "",
    "text": "Code\nusing Plots\nusing Statistics, LinearAlgebra\nusing RDatasets, DataFrames\niris = RDatasets.dataset(\"datasets\", \"iris\")  # Iris Datas\nNames = names(iris)\nX = Matrix(iris[:,1:3])\nn,p = size(X)\n#\np1 = scatter(X[:,1],X[:,2],X[:,3], c=[:blue :red :green], group=iris.Species,xlabel = Names[1],ylabel=Names[2],zlabel=Names[3])\np2 = scatter(X[:,1],X[:,2], c=[:blue :red :green], group=iris.Species,xlabel = Names[1],ylabel=Names[2])\np3 = scatter(X[:,1],X[:,3], c=[:blue :red :green], group=iris.Species,xlabel = Names[1],ylabel=Names[3])\nplot(p1,p2,p3)",
    "crumbs": [
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "PCA.html#pca-on-iris-data",
    "href": "PCA.html#pca-on-iris-data",
    "title": "Principal component analysis (PCA)",
    "section": "PCA on Iris Data",
    "text": "PCA on Iris Data\n\n\nCode\nusing MultivariateStats\n\nmodel = fit(PCA, X', maxoutdim=3, pratio = 0.99)  # \nU = projection(model)\nprintln(\"U = \")\ndisplay(U)\nΨ = MultivariateStats.transform(model, X')\nprintln(\"Ψ = \")\ndisplay(Ψ)\n\n\nU = \n\n\n3×3 Matrix{Float64}:\n  0.389833   0.639223   0.66289\n -0.091008   0.743059  -0.663009\n  0.916377  -0.198135  -0.347844\n\n\nΨ = \n\n\n3×150 Matrix{Float64}:\n -2.49088    -2.52334   -2.71115     …   1.58262     1.61255    1.25709\n  0.320973   -0.178401  -0.13782         0.0914441   0.157274  -0.272276\n  0.0339745   0.232901   0.00250557     -0.0282799  -0.561919  -0.39123",
    "crumbs": [
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "PCA.html#data",
    "href": "PCA.html#data",
    "title": "Principal component analysis (PCA)",
    "section": "Data",
    "text": "Data\n\n\nCode\nusing Plots\nusing Statistics, LinearAlgebra\nusing RDatasets, DataFrames\niris = RDatasets.dataset(\"datasets\", \"iris\")  # Iris Datas\nNames = names(iris)\nX = Matrix(iris[:,1:4])\np1 = scatter(X[:,1],X[:,2], c=[:blue :red :green], group=iris.Species,xlabel = Names[1],ylabel=Names[2])\np2 = scatter(X[:,3],X[:,4], c=[:blue :red :green], group=iris.Species,xlabel = Names[3],ylabel=Names[4])\nplot(p1,p2)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy PCA function\n\n\nCode\nusing LinearAlgebra, Statistics\nfunction my_PCA(X::Matrix{&lt;:Real})\n\"\"\"\n    Compute the PCA of Data\n    Input\n    X : (n,p) Matrix of reals\n         n = number of observations\n         p = number of variables\n    Output\n        Λ : Vector of the p eigen value in decrease order\n        U : (p,p) Matrix of reals\n            eigen vectors in column\n        Ψ : (n,p) Matrix of reals\n            Coordinates of the observation in the new basis\n        Φ = (p,p) Matrix of reals\n             Coordinates of the variables in the new basis\n        I_total : Real\n             total inertia\n        cum_var_ratio : p vector of reals\n             cumulative variance ratio\n\"\"\"\n     n,p = size(X)\n     # Calculation of centered data\n     xbar = mean(X,dims=1)\n     Xc = X - ones(n,1)*xbar\n     covMat = (1/n)*Xc'*Xc\n     # Computating total inertia\n     I_total = tr(covMat)\n     Λ, U = eigen(covMat)\n     eigOrder = sortperm(Λ, rev = true) # for abtaining increase order of eigen values\n     Λ = Λ[eigOrder]\n     # cumulative variance ratios\n     cum_var_ratio =  Vector{Float64}(undef,p)\n     for i in 1:p\n         cum_var_ratio[i] = sum(Λ[1:i])/I_total\n     end\n     U = U[:,eigOrder]\n     Ψ = Xc*U\n     Φ = U*sqrt.(n*diagm(Λ))\n     return Λ, U, Ψ, Φ, I_total,cum_var_ratio\nend\n\n\nmy_PCA (generic function with 1 method)\n\n\n\n\nPrint results\n\n\nCode\nmy_PCA_results = my_PCA(X)\nmy_Λ, my_U, my_Ψ, my_Φ, my_I_total, my_cum_var_ratio = my_PCA_results\nprintln(\"lambda = \", my_Λ)\nprintln(\"my_U = \")\ndisplay(my_U)\nprintln(\"my_Ψ = \")\ndisplay(my_Ψ)\nprintln(\"my_Φ = \")\ndisplay(my_Φ)\nprintln(\"Total inertia = \", my_I_total)\nprintln(\"my_cum_var_ratio = \", my_cum_var_ratio)\n\n\nlambda = [4.20005342799463, 0.2410529429424425, 0.07768810337596646, 0.023676192353626658]\nmy_U = \n\n\n4×4 Matrix{Float64}:\n -0.361387    0.656589   0.58203     0.315487\n  0.0845225   0.730161  -0.597911   -0.319723\n -0.856671   -0.173373  -0.0762361  -0.479839\n -0.358289   -0.075481  -0.545831    0.753657\n\n\nmy_Ψ = \n\n\n150×4 Matrix{Float64}:\n  2.68413   0.319397    0.0279148   0.00226244\n  2.71414  -0.177001    0.210464    0.0990266\n  2.88899  -0.144949   -0.0179003   0.0199684\n  2.74534  -0.318299   -0.0315594  -0.0755758\n  2.72872   0.326755   -0.0900792  -0.0612586\n  2.28086   0.74133    -0.168678   -0.0242009\n  2.82054  -0.0894614  -0.257892   -0.0481431\n  2.62614   0.163385    0.0218793  -0.0452979\n  2.88638  -0.578312   -0.0207596  -0.0267447\n  2.67276  -0.113774    0.197633   -0.0562954\n  2.50695   0.645069    0.075318   -0.0150199\n  2.61276   0.0147299  -0.10215    -0.156379\n  2.78611  -0.235112    0.206844   -0.00788791\n  ⋮                                \n -1.16933  -0.16499    -0.281836    0.0204618\n -2.10761   0.372288   -0.0272911   0.210622\n -2.31415   0.183651   -0.322694    0.277654\n -1.92227   0.409203   -0.113587    0.505305\n -1.41524  -0.574916   -0.296323   -0.0153047\n -2.56301   0.277863   -0.29257     0.0579127\n -2.41875   0.304798   -0.504483    0.241091\n -1.94411   0.187532   -0.177825    0.426196\n -1.52717  -0.375317    0.121898    0.254367\n -1.76435   0.0788589  -0.130482    0.137001\n -1.90094   0.116628   -0.723252    0.0445953\n -1.39019  -0.282661   -0.36291    -0.155039\n\n\nmy_Φ = \n\n\n4×4 Matrix{Float64}:\n  -9.07079   3.94817    1.98686    0.594543\n   2.12151   4.39057   -2.04108   -0.602526\n -21.5024   -1.04252   -0.260246  -0.904268\n  -8.99304  -0.453878  -1.86329    1.42029\n\n\nTotal inertia = 4.5424706666666665\nmy_cum_var_ratio = [0.9246187232017269, 0.9776852063187946, 0.9947878161267244, 0.9999999999999998]\n\n\n\n\nGraph of the observations\n\n\nCode\np3 = scatter(my_Ψ[:,1],my_Ψ[:,2], c=[:blue :red :green], group=iris.Species,xlabel = \"PC1\", ylabel=\"PC2\")\np4 = scatter(my_Ψ[:,3],my_Ψ[:,4], c=[:blue :red :green], group=iris.Species,xlabel = \"PC1\", ylabel=\"PC2\")\nplot(p3,p4)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraph of the variables\n\n\nCode\npvar1 = plot()\nech = 1.1*maximum(abs.(my_Φ))\nfor i=1:4\n    plot!(pvar1,[0,my_Φ[i,1]], [0,my_Φ[i,2]], xlims=(-ech,ech), ylims=(-ech,ech), arrow=true, label=Names[i], legend=:bottomleft, xlabel=\"v1\", ylabel=\"v2\")\nend\n\npvar2 = plot()\nfor i=1:4\n    plot!(pvar2,[0,my_Φ[i,3]], [0,my_Φ[i,4]], xlims=(-ech,ech), ylims=(-ech,ech), arrow=true, label=Names[i], legend=:bottomleft, xlabel=\"v3\", ylabel=\"v4\")\nend\nplot(pvar1,pvar2)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith MultivariateStats package\n\n\nCode\nusing MultivariateStats\n\nmodel = fit(PCA, X', maxoutdim=4, pratio = 0.999)  # Each column of X is an observation\nU = projection(model)\nprintln(\"U = \")\ndisplay(U)\nΨ = MultivariateStats.transform(model, X')\nprintln(\"Ψ = \")\ndisplay(Ψ)\ndisplay(Ψ'-my_Ψ) # Each column of Ψ is an observation\ndisplay(U-my_U)\n\n\nU = \n\n\n4×4 Matrix{Float64}:\n -0.361387    0.656589   0.58203     0.315487\n  0.0845225   0.730161  -0.597911   -0.319723\n -0.856671   -0.173373  -0.0762361  -0.479839\n -0.358289   -0.075481  -0.545831    0.753657\n\n\nΨ = \n\n\n4×150 Matrix{Float64}:\n 2.68413      2.71414     2.88899    …  -1.76435    -1.90094    -1.39019\n 0.319397    -0.177001   -0.144949       0.0788589   0.116628   -0.282661\n 0.0279148    0.210464   -0.0179003     -0.130482   -0.723252   -0.36291\n 0.00226244   0.0990266   0.0199684      0.137001    0.0445953  -0.155039\n\n\n150×4 Matrix{Float64}:\n  0.0           2.22045e-16   0.0           2.21177e-16\n  0.0           1.66533e-16  -5.55112e-17   4.44089e-16\n  0.0           2.77556e-16   0.0           0.0\n  0.0           2.77556e-16   2.15106e-16  -1.11022e-16\n  0.0           3.33067e-16   2.22045e-16   0.0\n  0.0           3.33067e-16  -1.66533e-16   2.42861e-17\n  0.0           3.88578e-16   5.55112e-17  -4.30211e-16\n  0.0           3.33067e-16   2.22045e-16   1.11022e-16\n  0.0           3.33067e-16   2.22045e-16  -2.22045e-16\n -4.44089e-16   2.498e-16     1.38778e-16   4.30211e-16\n  0.0           2.22045e-16  -2.22045e-16   4.42354e-16\n  0.0           5.75928e-16   2.22045e-16  -1.11022e-16\n -4.44089e-16   2.77556e-16   8.32667e-17   4.31946e-16\n  ⋮                                        \n  0.0           2.77556e-17  -5.55112e-17  -6.00214e-16\n  0.0          -3.33067e-16  -4.68375e-16   1.66533e-16\n  0.0          -3.05311e-16  -6.10623e-16  -4.996e-16\n  0.0          -4.44089e-16  -1.02696e-15   0.0\n  0.0           0.0           1.11022e-16  -8.10116e-16\n  4.44089e-16  -5.55112e-17  -2.77556e-16  -4.3715e-16\n  0.0          -1.66533e-16  -5.55112e-16  -8.60423e-16\n  0.0          -4.71845e-16  -8.32667e-16  -3.33067e-16\n  0.0          -4.44089e-16  -3.46945e-16   1.66533e-16\n  0.0          -2.08167e-16  -2.77556e-16  -3.05311e-16\n  0.0           1.66533e-16  -2.22045e-16  -1.43635e-15\n  2.22045e-16   1.11022e-16   2.77556e-16  -8.32667e-16\n\n\n4×4 Matrix{Float64}:\n  0.0          -5.55112e-16  -6.66134e-16   1.38778e-15\n -1.38778e-17   5.55112e-16   3.33067e-16  -8.88178e-16\n  0.0           2.498e-16     9.15934e-16  -2.77556e-16\n  5.55112e-17  -2.22045e-16  -1.44329e-15  -9.99201e-16",
    "crumbs": [
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "PCA.html#iris-data",
    "href": "PCA.html#iris-data",
    "title": "Principal component analysis (PCA)",
    "section": "Iris Data",
    "text": "Iris Data\n\n\nCode\nusing Plots\nusing Statistics, LinearAlgebra\nusing RDatasets, DataFrames\niris = RDatasets.dataset(\"datasets\", \"iris\")  # Iris Datas\nNames = names(iris)\nX = Matrix(iris[:,1:4])\np1 = scatter(X[:,1],X[:,2], c=[:blue :red :green], group=iris.Species,xlabel = Names[1],ylabel=Names[2])\np2 = scatter(X[:,3],X[:,4], c=[:blue :red :green], group=iris.Species,xlabel = Names[3],ylabel=Names[4])\nplot(p1,p2)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy PCA function\n\n\nCode\nusing LinearAlgebra, Statistics\nfunction my_PCA(X::Matrix{&lt;:Real};normed=false)::Tuple{Vector{&lt;:Real},Matrix{&lt;:Real},Matrix{&lt;:Real},Matrix{&lt;:Real},Real,Vector{&lt;:Real},Matrix{&lt;:Real}}\n\"\"\"\n    Compute the PCA of Data\n    Input\n    X : (n,p) Matrix of reals\n         n = number of observations\n         p = number of variables\n    Output\n        Λ : Vector of the p eigen value in decrease order\n        U : (p,p) Matrix of reals\n            eigen vectors in column\n        Ψ : (n,p) Matrix of reals\n            Coordinates of the observation in the new basis\n        Φ = (p,p) Matrix of reals\n             Coordinates of the variables in the new basis\n        I_total : Real\n             total inertia\n        cum_var_ratio : p vector of reals\n             cumulative variance ratio\n\"\"\"\n     n,p = size(X)\n     Λ = zeros(p); U = zeros(p,p); Ψ = zeros(n,p); Φ = zeros(p,p); I_total=0; cum_var_ratio = zeros(p)\n     # Calculation of centered data\n     xbar = mean(X,dims=1)\n     Xc = X - ones(n,1)*xbar\n     covMat = (1/n)*Xc'*Xc\n     if normed == true\n     s=std(Xc,corrected=false,dims=1)\n     Y=(Xc)./(ones(n,1)*s);\n     covMat=(1/n)*Y'*Y\n     end\n\n\n     # Computating total inertia\n     I_total = tr(covMat)\n     Λ, U = eigen(covMat)\n     eigOrder = sortperm(Λ, rev = true) # for abtaining increase order of eigen values\n     Λ = Λ[eigOrder]\n     # cumulative variance ratios\n     cum_var_ratio =  Vector{Float64}(undef,p)\n     for i in 1:p\n         cum_var_ratio[i] = sum(Λ[1:i])/I_total\n     end\n     U = U[:,eigOrder]\n     if normed == true\n       Ψ = Y*U\n       Φ = U*sqrt.(diagm(Λ))\n     else\n       Ψ = Xc*U\n       Φ = U*sqrt.(n*diagm(Λ))\n     end\n     return Λ, U, Ψ, Φ, I_total,cum_var_ratio, covMat\nend\n\n\nmy_PCA (generic function with 1 method)\n\n\n\n\nPrint results\n\n\nCode\nmy_PCA_results = my_PCA(X)\nmy_Λ, my_U, my_Ψ, my_Φ, my_I_total, my_cum_var_ratio, my_cov_mat = my_PCA_results\nprintln(\"lambda = \", my_Λ)\nprintln(\"my_U = \")\ndisplay(my_U)\nprintln(\"my_Ψ = \")\ndisplay(my_Ψ)\nprintln(\"my_Φ = \")\ndisplay(my_Φ)\nprintln(\"Total inertia = \", my_I_total)\nprintln(\"my_cum_var_ratio = \", my_cum_var_ratio)\nprintln(\"Matrix of variance, covariance = \")\ndisplay(my_cov_mat)\n\n\nlambda = [4.20005342799463, 0.2410529429424425, 0.07768810337596646, 0.023676192353626658]\nmy_U = \n\n\n4×4 Matrix{Float64}:\n -0.361387    0.656589   0.58203     0.315487\n  0.0845225   0.730161  -0.597911   -0.319723\n -0.856671   -0.173373  -0.0762361  -0.479839\n -0.358289   -0.075481  -0.545831    0.753657\n\n\nmy_Ψ = \n\n\n150×4 Matrix{Float64}:\n  2.68413   0.319397    0.0279148   0.00226244\n  2.71414  -0.177001    0.210464    0.0990266\n  2.88899  -0.144949   -0.0179003   0.0199684\n  2.74534  -0.318299   -0.0315594  -0.0755758\n  2.72872   0.326755   -0.0900792  -0.0612586\n  2.28086   0.74133    -0.168678   -0.0242009\n  2.82054  -0.0894614  -0.257892   -0.0481431\n  2.62614   0.163385    0.0218793  -0.0452979\n  2.88638  -0.578312   -0.0207596  -0.0267447\n  2.67276  -0.113774    0.197633   -0.0562954\n  2.50695   0.645069    0.075318   -0.0150199\n  2.61276   0.0147299  -0.10215    -0.156379\n  2.78611  -0.235112    0.206844   -0.00788791\n  ⋮                                \n -1.16933  -0.16499    -0.281836    0.0204618\n -2.10761   0.372288   -0.0272911   0.210622\n -2.31415   0.183651   -0.322694    0.277654\n -1.92227   0.409203   -0.113587    0.505305\n -1.41524  -0.574916   -0.296323   -0.0153047\n -2.56301   0.277863   -0.29257     0.0579127\n -2.41875   0.304798   -0.504483    0.241091\n -1.94411   0.187532   -0.177825    0.426196\n -1.52717  -0.375317    0.121898    0.254367\n -1.76435   0.0788589  -0.130482    0.137001\n -1.90094   0.116628   -0.723252    0.0445953\n -1.39019  -0.282661   -0.36291    -0.155039\n\n\nmy_Φ = \n\n\n4×4 Matrix{Float64}:\n  -9.07079   3.94817    1.98686    0.594543\n   2.12151   4.39057   -2.04108   -0.602526\n -21.5024   -1.04252   -0.260246  -0.904268\n  -8.99304  -0.453878  -1.86329    1.42029\n\n\nTotal inertia = 4.5424706666666665\nmy_cum_var_ratio = [0.9246187232017269, 0.9776852063187946, 0.9947878161267244, 0.9999999999999998]\nMatrix of variance, covariance = \n\n\n4×4 Matrix{Float64}:\n  0.681122   -0.0421511   1.26582    0.512829\n -0.0421511   0.188713   -0.327459  -0.120828\n  1.26582    -0.327459    3.0955     1.28697\n  0.512829   -0.120828    1.28697    0.577133\n\n\n\n\nGraph of the observations\n\n\nCode\np3 = scatter(my_Ψ[:,1],my_Ψ[:,2], c=[:blue :red :green], group=iris.Species,xlabel = \"PC1\", ylabel=\"PC2\")\np4 = scatter(my_Ψ[:,3],my_Ψ[:,4], c=[:blue :red :green], group=iris.Species,xlabel = \"PC1\", ylabel=\"PC2\")\nplot(p3,p4)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraph of the variables\n\n\nCode\npvar1 = plot()\nech = 1.1*maximum(abs.(my_Φ))\nfor i=1:4\n    plot!(pvar1,[0,my_Φ[i,1]], [0,my_Φ[i,2]], xlims=(-ech,ech), ylims=(-ech,ech), arrow=true, label=Names[i], legend=:bottomleft, xlabel=\"v1\", ylabel=\"v2\")\nend\n\npvar2 = plot()\nfor i=1:4\n    plot!(pvar2,[0,my_Φ[i,3]], [0,my_Φ[i,4]], xlims=(-ech,ech), ylims=(-ech,ech), arrow=true, label=Names[i], legend=:bottomleft, xlabel=\"v3\", ylabel=\"v4\")\nend\nplot(pvar1,pvar2)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith MultivariateStats package\n\n\nCode\nusing MultivariateStats\n\nmodel = fit(PCA, X', maxoutdim=4, pratio = 0.999)  # Each column of X is an observation\nU = projection(model)\nprintln(\"U = \")\ndisplay(U)\nΨ = MultivariateStats.transform(model, X')\nprintln(\"Ψ = \")\ndisplay(Ψ)\ndisplay(Ψ'-my_Ψ) # Each column of Ψ is an observation\ndisplay(U-my_U)\n\n\nU = \n\n\n4×4 Matrix{Float64}:\n -0.361387    0.656589   0.58203     0.315487\n  0.0845225   0.730161  -0.597911   -0.319723\n -0.856671   -0.173373  -0.0762361  -0.479839\n -0.358289   -0.075481  -0.545831    0.753657\n\n\nΨ = \n\n\n4×150 Matrix{Float64}:\n 2.68413      2.71414     2.88899    …  -1.76435    -1.90094    -1.39019\n 0.319397    -0.177001   -0.144949       0.0788589   0.116628   -0.282661\n 0.0279148    0.210464   -0.0179003     -0.130482   -0.723252   -0.36291\n 0.00226244   0.0990266   0.0199684      0.137001    0.0445953  -0.155039\n\n\n150×4 Matrix{Float64}:\n  0.0           2.22045e-16   0.0           2.21177e-16\n  0.0           1.66533e-16  -5.55112e-17   4.44089e-16\n  0.0           2.77556e-16   0.0           0.0\n  0.0           2.77556e-16   2.15106e-16  -1.11022e-16\n  0.0           3.33067e-16   2.22045e-16   0.0\n  0.0           3.33067e-16  -1.66533e-16   2.42861e-17\n  0.0           3.88578e-16   5.55112e-17  -4.30211e-16\n  0.0           3.33067e-16   2.22045e-16   1.11022e-16\n  0.0           3.33067e-16   2.22045e-16  -2.22045e-16\n -4.44089e-16   2.498e-16     1.38778e-16   4.30211e-16\n  0.0           2.22045e-16  -2.22045e-16   4.42354e-16\n  0.0           5.75928e-16   2.22045e-16  -1.11022e-16\n -4.44089e-16   2.77556e-16   8.32667e-17   4.31946e-16\n  ⋮                                        \n  0.0           2.77556e-17  -5.55112e-17  -6.00214e-16\n  0.0          -3.33067e-16  -4.68375e-16   1.66533e-16\n  0.0          -3.05311e-16  -6.10623e-16  -4.996e-16\n  0.0          -4.44089e-16  -1.02696e-15   0.0\n  0.0           0.0           1.11022e-16  -8.10116e-16\n  4.44089e-16  -5.55112e-17  -2.77556e-16  -4.3715e-16\n  0.0          -1.66533e-16  -5.55112e-16  -8.60423e-16\n  0.0          -4.71845e-16  -8.32667e-16  -3.33067e-16\n  0.0          -4.44089e-16  -3.46945e-16   1.66533e-16\n  0.0          -2.08167e-16  -2.77556e-16  -3.05311e-16\n  0.0           1.66533e-16  -2.22045e-16  -1.43635e-15\n  2.22045e-16   1.11022e-16   2.77556e-16  -8.32667e-16\n\n\n4×4 Matrix{Float64}:\n  0.0          -5.55112e-16  -6.66134e-16   1.38778e-15\n -1.38778e-17   5.55112e-16   3.33067e-16  -8.88178e-16\n  0.0           2.498e-16     9.15934e-16  -2.77556e-16\n  5.55112e-17  -2.22045e-16  -1.44329e-15  -9.99201e-16",
    "crumbs": [
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "PCA.html#section",
    "href": "PCA.html#section",
    "title": "Principal component analysis (PCA)",
    "section": "",
    "text": "Principal component analysis (PCA) reduces the number of dimensions in large datasets to principal components that retain most of the original information. It does this by transforming potentially correlated variables into a smaller set of uncorrelated variables, called principal components.\nLet \\(X\\) the matrix \\((n,p)\\) of data, we note \\(X_c\\) = the centered matrix. Then the empirical variances, covariances matrix is \\(C = \\frac{1}{n}X_c^TX_c\\). We note \\(\\Lambda\\) the vector of the eigen value (in decrease order) of the matrix \\(C\\) and \\(U\\) the \\((p,p)\\) orthogonal matrix of the eigen vectors : \\[C=U\\lambda U^T\\] Then We have * The coordinates of the \\(n\\) observations in the new basis of the eigen vectors \\((\\vec{u}_1,\\ldots,\\vec{u}_p)\\) are \\(\\Psi = X_cU\\) * The The coordinates of the \\(p\\) variables in the new basis of the eigen vectors \\((\\vec{v}_1,\\ldots,\\vec{v}_p)\\) are \\(\\Phi = \\sqrt(n)Udiad(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda}_p\\) * The total inertia (variance) is $=_{i=1,p}_i - The variance of the variable \\(v_i\\) is \\(\\lambda_i\\)",
    "crumbs": [
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "PCA.html#introduction",
    "href": "PCA.html#introduction",
    "title": "Principal component analysis (PCA)",
    "section": "",
    "text": "Principal component analysis (PCA) reduces the number of dimensions in large datasets to principal components that retain most of the original information. It does this by transforming potentially correlated variables into a smaller set of uncorrelated variables, called principal components.\nLet \\(X\\) the matrix \\((n,p)\\) of data, we note \\(X_c\\) = the centered matrix. Then the empirical variances, covariances matrix is \\(C = \\frac{1}{n}X_c^TX_c\\). We note \\(\\Lambda\\) the vector of the eigen value (in decrease order) of the matrix \\(C\\) and \\(U\\) the \\((p,p)\\) orthogonal matrix of the eigen vectors : \\[C=U\\texttt{diag}(\\Lambda) U^T.\\] Then We have\n\nThe coordinates of the \\(n\\) observations in the new basis of the eigen vectors \\((\\vec{u}_1,\\ldots,\\vec{u}_p)\\) are \\[\\Psi = X_cU\\]\nThe The coordinates of the \\(p\\) variables in the new basis of the eigen vectors \\((\\vec{v}_1,\\ldots,\\vec{v}_p)\\) are \\[\\Phi = \\sqrt{n}U\\texttt{diag}(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda}_p)\\]\nThe total inertia (variance) is \\[I = \\texttt{trace}(C)=\\sum_{i=1,p}\\lambda_i\\]\nThe variance of the variable \\(v_i\\) is \\(\\lambda_i\\)",
    "crumbs": [
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "TP-PCA.html#introduction",
    "href": "TP-PCA.html#introduction",
    "title": "Principal component analysis (PCA)",
    "section": "",
    "text": "Principal component analysis (PCA) reduces the number of dimensions in large datasets to principal components that retain most of the original information. It does this by transforming potentially correlated variables into a smaller set of uncorrelated variables, called principal components.\nLet \\(X\\) the matrix \\((n,p)\\) of data, we note \\(X_c\\) = the centered matrix. Then the empirical variances, covariances matrix is \\(C = \\frac{1}{n}X_c^TX_c\\). We note \\(\\Lambda\\) the vector of the eigen value (in decrease order) of the matrix \\(C\\) and \\(U\\) the \\((p,p)\\) orthogonal matrix of the eigen vectors : \\[C=U\\texttt{diag}(\\Lambda) U^T.\\] Then We have\n\nThe coordinates of the \\(n\\) observations in the new basis of the eigen vectors \\((\\vec{u}_1,\\ldots,\\vec{u}_p)\\) are \\[\\Psi = X_cU\\]\nThe The coordinates of the \\(p\\) variables in the new basis of the eigen vectors \\((\\vec{v}_1,\\ldots,\\vec{v}_p)\\) are \\[\\Phi = \\sqrt{n}U\\texttt{diag}(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda}_p)\\]\nThe total inertia (variance) is \\[I = \\texttt{trace}(C)=\\sum_{i=1,p}\\lambda_i\\]\nThe variance of the variable \\(v_i\\) is \\(\\lambda_i\\)"
  },
  {
    "objectID": "TP-PCA.html#iris-data",
    "href": "TP-PCA.html#iris-data",
    "title": "Principal component analysis (PCA)",
    "section": "Iris Data",
    "text": "Iris Data\n\n\nCode\nusing Plots\nusing Statistics, LinearAlgebra\nusing RDatasets, DataFrames\niris = RDatasets.dataset(\"datasets\", \"iris\")  # Iris Datas\nNames = names(iris)\nX = Matrix(iris[:,1:4])\np1 = scatter(X[:,1],X[:,2], c=[:blue :red :green], group=iris.Species,xlabel = Names[1],ylabel=Names[2])\np2 = scatter(X[:,3],X[:,4], c=[:blue :red :green], group=iris.Species,xlabel = Names[3],ylabel=Names[4])\nplot(p1,p2)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy PCA function\n\n\nCode\n\nusing LinearAlgebra, Statistics\nfunction my_PCA(X::Matrix{&lt;:Real})\n\"\"\"\n    Compute the PCA of Data\n    Input\n    X : (n,p) Matrix of reals\n         n = number of observations\n         p = number of variables\n    Output\n        Λ : Vector of the p eigen value in decrease order\n        U : (p,p) Matrix of reals\n            eigen vectors in column\n        Ψ : (n,p) Matrix of reals\n            Coordinates of the observation in the new basis\n        Φ = (p,p) Matrix of reals\n             Coordinates of the variables in the new basis\n        I_total : Real\n             total inertia\n        cum_var_ratio : p vector of reals\n             cumulative variance ratio\n\"\"\"\n    \n     return Λ, U, Ψ, Φ, I_total,cum_var_ratio\nend\n```\n\n### Print results\n```{julia}\nmy_PCA_results = my_PCA(X)\nmy_Λ, my_U, my_Ψ, my_Φ, my_I_total, my_cum_var_ratio = my_PCA_results\nprintln(\"lambda = \", my_Λ)\nprintln(\"my_U = \")\ndisplay(my_U)\nprintln(\"my_Ψ = \")\ndisplay(my_Ψ)\nprintln(\"my_Φ = \")\ndisplay(my_Φ)\nprintln(\"Total inertia = \", my_I_total)\nprintln(\"my_cum_var_ratio = \", my_cum_var_ratio)\n```\n\n### Graph of the observations\n\n```{julia}\np3 = scatter(my_Ψ[:,1],my_Ψ[:,2], c=[:blue :red :green], group=iris.Species,xlabel = \"PC1\", ylabel=\"PC2\")\np4 = scatter(my_Ψ[:,3],my_Ψ[:,4], c=[:blue :red :green], group=iris.Species,xlabel = \"PC1\", ylabel=\"PC2\")\nplot(p3,p4)\n```\n\n\n### Graph of the variables\n```{julia}\npvar1 = plot()\nech = 1.1*maximum(abs.(my_Φ))\nfor i=1:4\n    plot!(pvar1,[0,my_Φ[i,1]], [0,my_Φ[i,2]], xlims=(-ech,ech), ylims=(-ech,ech), arrow=true, label=Names[i], legend=:bottomleft, xlabel=\"v1\", ylabel=\"v2\")\nend\n\npvar2 = plot()\nfor i=1:4\n    plot!(pvar2,[0,my_Φ[i,3]], [0,my_Φ[i,4]], xlims=(-ech,ech), ylims=(-ech,ech), arrow=true, label=Names[i], legend=:bottomleft, xlabel=\"v3\", ylabel=\"v4\")\nend\nplot(pvar1,pvar2)\n```\n\n\n\n### With MultivariateStats package\n\n```{julia}\nusing MultivariateStats\n\nmodel = fit(PCA, X', maxoutdim=4, pratio = 0.999)  # Each column of X is an observation\nU = projection(model)\nprintln(\"U = \")\ndisplay(U)\nΨ = MultivariateStats.transform(model, X')\nprintln(\"Ψ = \")\ndisplay(Ψ)\ndisplay(Ψ'-my_Ψ) # Each column of Ψ is an observation\ndisplay(U-my_U)\n```"
  },
  {
    "objectID": "PCA.html#normed-pca",
    "href": "PCA.html#normed-pca",
    "title": "Principal component analysis (PCA)",
    "section": "Normed PCA",
    "text": "Normed PCA\n\nIntroduction\n\n\\(X\\) the matrix \\((n,p)\\) of data\n\\(X_c\\) is the centered matrix\n\\(Y\\) is the centered and normed matrix. Each column of \\(Xc\\) is divided by its sample standard deviation\n\\(R=Y^TY\\) is the corretalion matrix of the Data \\(X\\).\n\\[C=U\\texttt{diag}(\\Lambda) U^T.\\] Then We have\nThe coordinates of the \\(n\\) observations in the new basis of the eigen vectors \\((\\vec{u}_1,\\ldots,\\vec{u}_p)\\) are \\[\\Psi = YU\\]\nThe The coordinates of the \\(p\\) variables in the new basis of the eigen vectors \\((\\vec{v}_1,\\ldots,\\vec{v}_p)\\) are \\[\\Phi = U\\texttt{diag}(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda}_p)\\]\nThe total inertia (variance) is \\[I = \\texttt{trace}(C)=\\sum_{i=1,p}\\lambda_i\\]\nThe variance of the variable \\(v_i\\) is \\(\\lambda_i\\)\n\n\n\nData\n\n\nCode\n# Data from Tomassone page 138 : mineral waters\nX =[341   27   3   84   23   2  \n263   23   9   91   5   3  \n287   3   5   44   24   23  \n   298   9   23   96   6   11 \n    200   15   8   70   2   4 \n    250   5   20   71   6   11 \n   357   10   2   78   24   5 \n      311   14   18   73   18   13 \n    256   6   23   86   3   18  \n   186   10   16   64   4   9 \n    183   16   44   48   11   31 \n       398   218   15   157   35   8 \n      348   51   31   140   4   14 \n   168   24   8   55   5   9 \n   110   65   5   4   1   3 \n   332   14   8   103   16   5 \n      196   18   6   58   6   13 \n       59   7   6   16   2   9 \n       402   306   15   202   36   3 \n       64   7   8   10   6   8 ]\n\ndf = DataFrame(X,[:HCO3, :SO4, :Cl, :Ca, :Mg, :Na])\ndf[:, :Origins] = [\"Aix-les-Bains\", \"Beckerish\",\n\"Cayranne\", \n\"Chambon\",\n\"Cristal-Roc\",\n\"St Cyr\",\n\"Evian\",\n\"Ferita\",\n\"St Hyppolite\",\n\"Laurier\", \n\"Ogeu\",\n\"Ondine\",\n\"Perrier\",\n\"Ribes\", \n\"Spa\",\n\"Thonon\", \n\"Veri\", \n\"Viladreau\",\n\"Vittel\", \n\"Volvic\"]\n\n\n20-element Vector{String}:\n \"Aix-les-Bains\"\n \"Beckerish\"\n \"Cayranne\"\n \"Chambon\"\n \"Cristal-Roc\"\n \"St Cyr\"\n \"Evian\"\n \"Ferita\"\n \"St Hyppolite\"\n \"Laurier\"\n \"Ogeu\"\n \"Ondine\"\n \"Perrier\"\n \"Ribes\"\n \"Spa\"\n \"Thonon\"\n \"Veri\"\n \"Viladreau\"\n \"Vittel\"\n \"Volvic\"\n\n\n\n\nGraph of the observations\n\n\nCode\nmy_PCA_results = my_PCA(X,normed=true)\nmy_Λ, my_U, my_Ψ, my_Φ, my_I_total, my_cum_var_ratio, my_R = my_PCA_results\np3 = scatter(my_Ψ[:,1],my_Ψ[:,2],xlabel = \"PC1\", ylabel=\"PC2\")\np4 = scatter(my_Ψ[:,3],my_Ψ[:,4], xlabel = \"PC1\", ylabel=\"PC2\")\nplot(p3,p4)\n\nprintln(sum(my_Φ.^2,dims=1))\nprintln(sum(my_Φ.^2,dims=2))\nprintln(\"my_Φ*my_Φ' = \")\ndisplay(my_Φ*my_Φ')\nprintln(\"Matrix of correlation = \")\ndisplay(my_R)\nprintln(\"my_Φ'*my_Φ = \")\ndisplay(my_Φ'*my_Φ)\n\n\n[3.0940874688062916 1.6875603215911315 0.596513190404363 0.5028441637469867 0.09323922489032915 0.025755630560899263]\n[0.9999999999999997; 0.9999999999999998; 1.0000000000000002; 0.9999999999999998; 1.0000000000000002; 1.0000000000000018;;]\nmy_Φ*my_Φ' = \n\n\n6×6 Matrix{Float64}:\n  1.0        0.47796     0.121776    0.851749   0.730575   -0.108825\n  0.47796    1.0         0.0449271   0.732646   0.670631   -0.278783\n  0.121776   0.0449271   1.0         0.252035  -0.125463    0.668014\n  0.851749   0.732646    0.252035    1.0        0.605682   -0.196195\n  0.730575   0.670631   -0.125463    0.605682   1.0        -0.0905507\n -0.108825  -0.278783    0.668014   -0.196195  -0.0905507   1.0\n\n\nMatrix of correlation = \n\n\n6×6 Matrix{Float64}:\n  1.0        0.47796     0.121776    0.851749   0.730575   -0.108825\n  0.47796    1.0         0.0449271   0.732646   0.670631   -0.278783\n  0.121776   0.0449271   1.0         0.252035  -0.125463    0.668014\n  0.851749   0.732646    0.252035    1.0        0.605682   -0.196195\n  0.730575   0.670631   -0.125463    0.605682   1.0        -0.0905507\n -0.108825  -0.278783    0.668014   -0.196195  -0.0905507   1.0\n\n\nmy_Φ'*my_Φ = \n\n\n6×6 Matrix{Float64}:\n  3.09409       1.37651e-16  -7.61115e-17  …   1.16191e-16   6.47272e-17\n  1.37651e-16   1.68756       2.83071e-16      9.92044e-17  -1.7331e-17\n -7.61115e-17   2.83071e-16   0.596513        -2.92013e-18   9.16626e-18\n -7.03885e-17   2.09074e-16  -8.87984e-17     -1.30018e-17  -1.25156e-17\n  1.16191e-16   9.92044e-17  -2.92013e-18      0.0932392     5.0468e-18\n  6.47272e-17  -1.7331e-17    9.16626e-18  …   5.0468e-18    0.0257556\n\n\n\n\nGraph of the variables\n\n\nCode\nn,p = size(X)\nprintln(\"p = \", p)\nNames = names(df)\nprintln(Names)\npvar1 = plot()\nech = 1.1*maximum(abs.(my_Φ))\nfor i=1:p\n    plot!(pvar1,[0,my_Φ[i,1]], [0,my_Φ[i,2]], xlims=(-ech,ech), ylims=(-ech,ech), arrow=true, label=Names[i], legend=:bottomleft, xlabel=\"v1\", ylabel=\"v2\")\nend\n\n# Plot the unit cercle\ncercle(θ)=[cos.(θ) sin.(θ)]\nInter_theta = 0:0.01:2*π\nCercle = cercle(Inter_theta)\nplot!(pvar1,Cercle[:,1],Cercle[:,2])\n\npvar2 = plot()\nfor i=1:p\n    plot!(pvar2,[0,my_Φ[i,3]], [0,my_Φ[i,4]], xlims=(-ech,ech), ylims=(-ech,ech), arrow=true, label=Names[i], legend=:bottomleft, xlabel=\"v3\", ylabel=\"v4\")\nend\nplot!(pvar2,Cercle[:,1],Cercle[:,2])\nplot(pvar1,pvar2,aspect_ratio=:equal)\n\n\np = 6\n[\"HCO3\", \"SO4\", \"Cl\", \"Ca\", \"Mg\", \"Na\", \"Origins\"]",
    "crumbs": [
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "image.html",
    "href": "image.html",
    "title": "Cours",
    "section": "",
    "text": "Code\nusing Plots\nusing TestImages, Images\nimg = testimage(\"lighthouse\")\nimgarr = channelview(img)\nplot(img)\nprintln(imgarr[1,1:3,1:3])\nimg_red = StackedView(imgarr[1,:,:], zeroarray, zeroarray)\nimg2_red = colorview(RGB, img_red)\nimg_green = StackedView(zeroarray,imgarr[2,:,:], zeroarray)\nimg2_green = colorview(RGB, img_green)\nimg_blue = StackedView(zeroarray, zeroarray,imgarr[3,:,:])\nimg2_blue = colorview(RGB, img_blue)\n#permutedims(imgarr, [2,3,1])\nmosaicview(img,img2_red, img2_green, img2_blue; nrow = 2)\n\n\nN0f8[0.361N0f8 0.349N0f8 0.345N0f8; 0.361N0f8 0.361N0f8 0.361N0f8; 0.376N0f8 0.353N0f8 0.361N0f8]\n\n\n\n\n\n\n\nCode\n\nX1=imgarr[1,:,:];X2=imgarr[2,:,:];X3=imgarr[3,:,:];\nnl,nc=size(X1);\nprintln(nl,\" \", nc)\n#\n# X is the matrix for the PCA\nX=[X1[:] X2[:] X3[:]];\nprintln(size(X))\nusing LinearAlgebra, Statistics\nfunction my_PCA(X::Matrix{&lt;:Real};normed=false)::Tuple{Vector{&lt;:Real},Matrix{&lt;:Real},Matrix{&lt;:Real},Matrix{&lt;:Real},Real,Vector{&lt;:Real}}\n\"\"\"\n    Compute the PCA of Data\n    Input\n    X : (n,p) Matrix of reals\n         n = number of observations\n         p = number of variables\n    Output\n        Λ : Vector of the p eigen value in decrease order\n        U : (p,p) Matrix of reals\n            eigen vectors in column\n        Ψ : (n,p) Matrix of reals\n            Coordinates of the observation in the new basis\n        Φ = (p,p) Matrix of reals\n             Coordinates of the variables in the new basis\n        I_total : Real\n             total inertia\n        cum_var_ratio : p vector of reals\n             cumulative variance ratio\n\"\"\"\n     n,p = size(X)\n     Λ = zeros(p); U = zeros(p,p); Ψ = zeros(n,p); Φ = zeros(p,p); I_total=0; cum_var_ratio = zeros(p)\n     # Calculation of centered data\n     xbar = mean(X,dims=1)\n     Xc = X - ones(n,1)*xbar\n     covMat = (1/n)*Xc'*Xc\n     if normed == true\n     s=std(Xc,corrected=false,dims=1)\n     Y=(Xc)./(ones(n,1)*s);\n     covMat=(1/n)*Y'*Y\n     println(\"R = \", covMat)\n     end\n\n\n     # Computating total inertia\n     I_total = tr(covMat)\n     Λ, U = eigen(covMat)\n     eigOrder = sortperm(Λ, rev = true) # for abtaining increase order of eigen values\n     Λ = Λ[eigOrder]\n     # cumulative variance ratios\n     cum_var_ratio =  Vector{Float64}(undef,p)\n     for i in 1:p\n         cum_var_ratio[i] = sum(Λ[1:i])/I_total\n     end\n     U = U[:,eigOrder]\n     if normed == true\n       Ψ = Y*U\n       Φ = U*sqrt.(diagm(Λ))\n     else\n       Ψ = Xc*U\n       Φ = U*sqrt.(n*diagm(Λ))\n     end\n     return Λ, U, Ψ, Φ, I_total,cum_var_ratio\nend\n\nΛ, U, Ψ, Φ, I_total, cum_var_ratio = my_PCA(X)\nprintln(\"nl = \", nl)\nprintln(\"nc = \", nc)\nprintln(size(Ψ))\nΨ1 = zeros(nl,nc)\nΨ2 = zeros(nl,nc)\nΨ3 = zeros(nl,nc)\nprintln(size(Ψ1))\nI=1:nl;\nfor j in 1:nc\n   Ψ1[I,j] = Ψ[(j-1)*nl .+ I,1];\n   Ψ2[I,j] = Ψ[(j-1)*nl .+ I,2];\n   Ψ3[I,j] = Ψ[(j-1)*nl .+ I,3];\nend\n\n\nminΨ1=minimum(Ψ1)\nmaxΨ1=maximum(Ψ1)\nPSI1=(Ψ1 .- minΨ1) ./ (maxΨ1-minΨ1)\n\nusing Colors\nGray.(PSI1)\n#@view PSI1\n\n\n512 768\n(393216, 3)\nnl = 512\nnc = 768\n(393216, 3)\n(512, 768)"
  },
  {
    "objectID": "optimization.html",
    "href": "optimization.html",
    "title": "Optimization",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbf{R}}}\n\\newcommand{\\abs}[1]{\\lvert#1\\rvert}\n\\newcommand{\\norme}[1]{\\lVert#1\\rVert}\n\\]",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "optimization.html#algorithms",
    "href": "optimization.html#algorithms",
    "title": "Optimization",
    "section": "Algorithms",
    "text": "Algorithms\nSteepest Descent Algorithm: Step 0. Given x0, set k := 0 Step 1. dk := −∇f(xk). If dk = 0, then stop. Step 2. Solve minα f(xk + αdk) for the stepsize αk, perhaps chosen by an exact or inexact linesearch. Step 3. Set xk+1 ← xk + αkdk, k ← k + 1. Go to Step 1",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "optimization.html#introduction",
    "href": "optimization.html#introduction",
    "title": "Optimization",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\nPierre de Fermat\n\n\n\n\n\nPierre de Fermat, 1601 (Beaumont-de-Lomagne, near Montauban) – 1665 (Castres)\nmethod maximis et minimis\nEarly developments that led to infinitesimal calculus\n\n\n\n\nThe problem is to solve\n\\[(P)\\left\\{\\begin{array}{l}\nMin\\; f(x)\\\\\nx\\in\\R^n\n\\end{array}\\right.\n\\]",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "optimization.html#newtons-algorithm",
    "href": "optimization.html#newtons-algorithm",
    "title": "Optimization",
    "section": "Newton’s Algorithm",
    "text": "Newton’s Algorithm\nSolve \\(\\nabla f(x) = 0\\) by Newton’s method\nRequire \\(f \\colon \\R^n \\to \\R\\), \\(x_{0} \\in \\R^n\\) ( initial point)\n\n\\(k \\gets 0\\)\ncontinue = true\nWhile continue \\(\\#\\) See Section Section 1.3.1\n\n$d_k $ solution of \\(\\nabla ^2f(x_k)\\, d =-\\nabla f(x\\_k)\\) \\(\\#\\) Newton’s direction\n\\(x_{k+1} \\gets x_{k} + d_{k}\\) # Mise à jour de l’itéré\n\\(k \\gets k + 1\\)\ncontinue = stop_function(\\(\\nabla f_k\\),\\(x_k\\),\\(x_{k+1}\\),\\(f_k\\),\\(f_{k+1}\\),AbsTol,RelTol,\\(\\varepsilon\\))\n\nEndWhile\n\n\nStop criteria\n\nStop criteria, Stagnation criteria are more restrictive (\\(\\varepsilon = 0.01\\), for example).\n\n\n\n\n\n\nCriteria\nFormula\n\n\n\n\n\\(\\norme{\\nabla f(x_{k+1})}=0\\)\n\\(\\norme{\\nabla f(x_{k+1})} &lt; \\mathrm{max}(\\mathrm{RelTol}\\norme{\\nabla f(x_0)},\\mathrm{AbsTol})\\)\n\n\nStagnation of the iterate\n\\(\\norme{x_{k+1}-x_k} &lt; \\varepsilon\\mathrm{max}(\\mathrm{RelTol}\\norme{x_{k+1}},\\mathrm{AbsTol})\\)\n\n\nStagnation of the function\n\\(\\abs{f(x_{k+1})-f(x_k)} &lt; \\varepsilon\\mathrm{max}(\\mathrm{RelTol}\\abs{f(x_{k+1})},\\mathrm{AbsTol})\\)\n\n\nMaximum number of iteration\n\\(k+1 = \\mathrm{max\\_iter}\\)\n\n\n\n\n\nExercice\n\n\nCode\nusing LinearAlgebra\nusing ForwardDiff\n\n\"\"\"\n   Solve by Newton's algorithm the optimization problem Min f(x)\n   Case where f is a function from R^n to R\n\"\"\"\n\n\nfunction algo_Newton(f,x0::Vector{&lt;:Real};AbsTol= abs(eps()), RelTol = abs(eps()), ε=0.01, nbit_max = 0)\n# to complete\n    \n    # flag = 0 if the program stop on the first criteria\n    # flag = 2 if the program stop on the second criteria\n    # ...\n    return xₖ, flag, fₖ, ∇fₖ , k  \nend\n\n\nalgo_Newton (generic function with 1 method)\n\n\n\n\nCode\ninclude(\"src/MyOptims.jl\")\n\nA = [1 0 ; 0 9/2]\nb = [0,0]; c=0.\nf1(x) = 0.5*x'*A*x + b'*x +c\nx0 = [1000,-20]\nprintln(\"Results for Newton on f1 : \", algo_Newton(f1,x0))\nprintln(\"eigen value of 0.5(A^T+A) = \", 0.5*eigen(A'+A).values)\nusing Plots;\nx=range(-10,stop=10,length=100)\ny=range(-10,stop=10,length=100)\nf11(x,y) = f1([x,y])\np1 = plot(x,y,f11,st=:contourf)\n\nA = [-1 0 ; 0 3]\nf2(x) = 0.5*x'*A*x + b'*x +c\nprintln(\"Results for Newton on f2 : \", algo_Newton(f2,x0))\nprintln(\"eigen value of 0.5(A^T+A) = \", 0.5*eigen(A'+A).values)\nf21(x,y) = f1([x,y])\np2 = plot(x,y,f21,st=:contourf)\n# Rosenbrock function\nx=range(-1.5,stop=2,length=100)\ny=range(-0.5,stop=3.,length=100)\nf3(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2\nf31(x,y) = f3([x,y])\np3 = plot(x,y,f31,st=:contourf)\nx0 = [1.1,1.2]\n\nprintln(\"Results for Newton on f3 : \", algo_Newton(f3,[1.1,1.2]))\n\nprintln(\"Results for Newton on f3 : \", algo_Newton(f3,[3.,0.]))\nplot(p1,p2,p3)\n\n\nResults for Newton on f1 : ([0.0, 0.0], 0, 0.0, [0.0, 0.0], 1)\neigen value of 0.5(A^T+A) = [1.0, 4.5]\nResults for Newton on f2 : ([0.0, 0.0], 0, 0.0, [0.0, 0.0], 1)\neigen value of 0.5(A^T+A) = [-1.0, 3.0]\nResults for Newton on f3 : ([1.0, 1.0], 0, 0.0, [-0.0, 0.0], 7)\nResults for Newton on f3 : ([1.0, 1.0], 0, 0.0, [-0.0, 0.0], 5)",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "optimization.html#automatique-differentiation",
    "href": "optimization.html#automatique-differentiation",
    "title": "Optimization",
    "section": "Automatique differentiation",
    "text": "Automatique differentiation\n\n\nCode\nusing LinearAlgebra\nusing ForwardDiff\nA = [-1 2;1 4]; b=[1,1]; c=1\nf(x) = 0.5*x'*A*x + b'*x +c\n\nanalytic_∇f(x) = 0.5*(A' + A)*x+b\nanalytic_∇²f(x) = 0.5*(A' + A)\n\n∇f(x) = ForwardDiff.gradient(f, x)\n∇²f(x) = ForwardDiff.hessian(f, x)\n\nx0 = [1,-1]\nprintln(\"∇f(x0) = \", ∇f(x0))\nprintln(\"analytic_∇f(x0) = \", analytic_∇f(x0))\nprintln(\"analytic_∇f(x0) - ∇f(x0) = \", analytic_∇f(x0)- ∇f(x0))\n\nprintln(\"∇²f(x0) = \", ∇²f(x0))\nprintln(\"analytic_∇²f(x0) = \", analytic_∇²f(x0))\nprintln(\"analytic_∇²f(x0) - ∇²f(x0) = \", analytic_∇²f(x0)- ∇²f(x0))\n\n\n∇f(x0) = [-1.5, -1.5]\nanalytic_∇f(x0) = [-1.5, -1.5]\nanalytic_∇f(x0) - ∇f(x0) = [0.0, 0.0]\n∇²f(x0) = [-1.0 1.5; 1.5 4.0]\nanalytic_∇²f(x0) = [-1.0 1.5; 1.5 4.0]\nanalytic_∇²f(x0) - ∇²f(x0) = [0.0 0.0; 0.0 0.0]",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "PCA.html#colored-image-to-the-best-black-and-white",
    "href": "PCA.html#colored-image-to-the-best-black-and-white",
    "title": "Principal component analysis (PCA)",
    "section": "Colored Image to the best black and white",
    "text": "Colored Image to the best black and white\n\nData\n\n\nCode\nusing Plots\nusing TestImages, Images\nimg = testimage(\"lighthouse\")\nimgarr = channelview(img)\nplot(img)\nprintln(imgarr[1,1:3,1:3])\nimg_red = StackedView(imgarr[1,:,:], zeroarray, zeroarray)\nimg2_red = colorview(RGB, img_red)\nimg_green = StackedView(zeroarray,imgarr[2,:,:], zeroarray)\nimg2_green = colorview(RGB, img_green)\nimg_blue = StackedView(zeroarray, zeroarray,imgarr[3,:,:])\nimg2_blue = colorview(RGB, img_blue)\n#permutedims(imgarr, [2,3,1])\nmosaicview(img,img2_red, img2_green, img2_blue; nrow = 2)\n\n\nN0f8[0.361N0f8 0.349N0f8 0.345N0f8; 0.361N0f8 0.361N0f8 0.361N0f8; 0.376N0f8 0.353N0f8 0.361N0f8]\n\n\n\n\n\n\n\nPCA\n\n\nCode\nX1=imgarr[1,:,:];X2=imgarr[2,:,:];X3=imgarr[3,:,:];\nnl,nc=size(X1);\nprintln(nl,\" \", nc)\n#\n# X is the matrix for the PCA\nX=[X1[:] X2[:] X3[:]];\nprintln(size(X))\nΛ, U, Ψ, Φ, I_total, cum_var_ratio, cov_mat = my_PCA(X)\nprintln(\"nl = \", nl)\nprintln(\"nc = \", nc)\nprintln(size(Ψ))\nΨ1 = zeros(nl,nc)\nΨ2 = zeros(nl,nc)\nΨ3 = zeros(nl,nc)\nprintln(size(Ψ1))\nI=1:nl;\nfor j in 1:nc\n   Ψ1[I,j] = Ψ[(j-1)*nl .+ I,1];\n   Ψ2[I,j] = Ψ[(j-1)*nl .+ I,2];\n   Ψ3[I,j] = Ψ[(j-1)*nl .+ I,3];\nend\n\n\nminΨ1=minimum(Ψ1)\nmaxΨ1=maximum(Ψ1)\nPSI1=(Ψ1 .- minΨ1) ./ (maxΨ1-minΨ1)\n\nusing Colors\nGray.(PSI1)\n#@view PSI1\n\n\n512 768\n(393216, 3)\nnl = 512\nnc = 768\n(393216, 3)\n(512, 768)",
    "crumbs": [
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "optimization.html#steepest-descent",
    "href": "optimization.html#steepest-descent",
    "title": "Optimization",
    "section": "Steepest descent",
    "text": "Steepest descent\n\nAlgorithm\nRequire \\(f \\colon \\R^n \\to \\R\\), \\(x_{0} \\in \\R^n\\) ( initial point)\n\n\\(k \\gets 0\\)\ncontinue = true\nWhile continue \\(\\#\\) See Section Section 1.3.1\n\n\\(d_k = -\\nabla f(x_k)\\)\n\\(\\alpha_k = argmin_{\\alpha&gt;0}\\{f(x_k+\\alpha d_k)\\}\\)\n\\(x_{k+1} \\gets x_{k} + \\alpha_kd_{k}\\)\n\n\\(k \\gets k + 1\\)\ncontinue = stop_function(\\(\\nabla f_k\\),\\(x_k\\),\\(x_{k+1}\\),\\(f_k\\),\\(f_{k+1}\\),AbsTol,RelTol,\\(\\varepsilon\\))\n\nEndWhile\n\n\n\nSteepest descent for a quadratic function\n\n\n\n\n\n\nExercise:\n\n\n\nLet \\(f(x)=x^TAx +b^Tx + c\\), where \\(A\\) symetric and positive-definite.\nIf \\(x_k\\) is a fixed vector and \\(d_k\\) a nonzero vector compute \\(\\alpha^*\\) the solution of \\[Min\\; g(\\alpha)=f(x_k+\\alpha d_k)\\]\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\\[\\alpha^* = -\\frac{2x_k^TAd_k + b^Td_k}{2d_k^TAd_k}\\]\n\n\n\n\n\n\n\n\n\nExercise:\n\n\n\nComplete the steepest_descent_quad function and execute the following sript and explain why the Newton’s algorithm converge in one iteration.\n\n\n\n\nA = [1.0 0.0; 0.0 9.0]\nb = [-4.0, -18.0]\nn= 5\n\nResult with Newton's algorithm : \nxsol = [2.0, 1.0]\nflag = 0\nfsol = 0.0\n∇f_xsol = [0.0, 0.0]\nnb_iter = 1\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear search: Armijo’s condition and backtraking\nLet \\(x_k\\) the current iterate and \\(d_k=-\\nabla f(x_k)\\). We note \\(g_k(\\alpha)=f(x_k+\\alpha d_k)\\) We want to find an \\(\\alpha&gt;0\\) such that we have a sufficient decay of the function \\(g_k\\), i.e. which verify the Armijo’s condition : \\(g_f(\\alpha)=f(x_k+\\alpha d_k)\\le f_k+c_1\\alpha\\nabla f_k^Td_k=\\tilde{g}_f(\\alpha)\\)\n\nInitialization\n\n\\(\\alpha_0=1\\)\n\\(\\rho\\in]0,1[\\) (\\(\\rho=0.8\\))\n\\(c\\in]0,1[\\) (\\(c=10^{-4}\\))\n\\(k=0\\)\n\nWhile \\(g_{f}(\\alpha_k)&gt;\\tilde{g}_f(\\alpha_k)=f_k+c_1\\alpha_k\\nabla f_k^Td_k\\)\n\n\\(\\alpha_k:=\\rho\\alpha_k\\)\n\\(k:=k+1\\)\n\nend\n\n\n\n\n\n\n\nExercise: Complete the descent_backtrac function and execute the following sript.\n\n\n\n\n\nCode\nusing Plots\nx = range(-10*(n/2)-xsol[1],stop=10*(n/2)-xsol[1],length=100)\ny = range(-9*(n/2)-xsol[2],stop=9*(n/2)-xsol[2],length=100)\nf_contour(x,y) = f([x,y])\nz = @. f_contour(x', y)\nnb_levels = 7\nx0 = (n/2)*[9,1]\nxₖ = x0\np1 = plot()\nfor nbit in 1:nb_levels\n  xsol, flag, fsol, ∇f_xsol , nb_iter  = descent_backtrac(f,x0,nbit_max=nbit)\n  #xsol, flag, fsol, ∇f_xsol , nb_iter  = my_descent(f,x0,nbit_max=nbit)\n  println(\"xsol = \", xsol)\n  println(\"flag = \", flag)\n  println(\"fsol = \", fsol)\n  println(\"∇f_xsol = \", ∇f_xsol)\n  println(\"nb_iter = \", nb_iter)\n  plot!(p1,[xₖ[1],xsol[1]],[xₖ[2],xsol[2]],arrow=true)\n  xₖ = xsol\nend\nlevels = [f(Xsol[k,:]) for k in nb_levels+1:-1:1]\ncontour!(p1,xx,yy,z,levels=levels,cbar=false,color=:turbo)\nplot(p1,legend=false)\n\n\nxsol = [11.752095999999996, -4.577888000000002]\nflag = 3\nfsol = 375.1188872581122\n∇f_xsol = [19.504191999999993, -100.40198400000004]\nnb_iter = 1\nxsol = [9.657849330627375, 6.2026929433378895]\nflag = 3\nfsol = 302.25478113451095\n∇f_xsol = [15.31569866125475, 93.648472980082]\nnb_iter = 2\nxsol = [8.013338708990371, -3.8527352759069133]\nflag = 3\nfsol = 248.1015993513241\n∇f_xsol = [12.026677417980743, -87.34923496632445]\nnb_iter = 3\nxsol = [6.721984054246145, 5.52631741186767]\nflag = 3\nfsol = 206.68507722534292\n∇f_xsol = [9.44396810849229, 81.47371341361807]\nnb_iter = 4\nxsol = [5.70794569998511, -3.221855953011487]\nflag = 3\nfsol = 174.16547050584487\n∇f_xsol = [7.415891399970221, -75.99340715420678]\nnb_iter = 5\nxsol = [4.911670424146116, 4.93787400796178]\nflag = 3\nfsol = 148.03948998207602\n∇f_xsol = [5.823340848292233, 70.88173214331205]\nnb_iter = 6\nxsol = [4.286393961724215, -2.6729940280221545]\nflag = 3\nfsol = 126.64556351718664\n∇f_xsol = [4.5727879234484305, -66.11389250439878]\nnb_iter = 7",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "TP-steepest-descent.html",
    "href": "TP-steepest-descent.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "Code\n\n\"\"\"\n   Solve Min f(x)=0.5x^TAx +b^Tx+c\n   by the steepest descent \n\"\"\"\nfunction steepest_descent_quad(A::Matrix{&lt;:Real},b::Vector{&lt;:Real},c::Real,x0::Vector{&lt;:Real};AbsTol= abs(eps()), RelTol = abs(eps()), ε=0.01, nbit_max = 0)\n  # To complete\n  \n  return xₖ, flag, fₖ, ∇fₖ, k  \nend\n\n\n\n\nCode\nA = [1 0 ; 0 9]\nb = [0,0]; c=0.\nf(x) = 0.5*x'*A*x + b'*x +c\n∇f(x) = 0.5*(A' + A)*x+b\nx = range(-9,stop=9,length=100)\ny = range(-9,stop=9,length=100)\nf_contour(x,y) = f([x,y])\nz = @. f_contour(x', y)\nnb_levels = 7\nlevels = [f(0.8^k*[9,(-1)^k]) for k in nb_levels:-1:0]\np1 = contour(x,y,z,levels=levels,cbar=false,color=:turbo)\nx0 = [9,1]\nprintln(\"itérés = \", [0.8^k*[9,(-1)^k] for k in 0:5])\nxₖ = x0\nfor nbit in 1:nb_levels\n  xsol, flag, fsol, ∇f_xsol , nb_iter  = steepest_descent(A,b,c,x0,nbit_max=nbit)\n  println(\"xsol = \", xsol)\n  println(\"flag = \", flag)\n  println(\"fsol = \", fsol)\n  println(\"∇f_xsol = \", ∇f_xsol)\n  println(\"nb_iter = \", nb_iter)\n  plot!(p1,[xₖ[1],xsol[1]],[xₖ[2],xsol[2]])\nxₖ = xsol\nend\n\nplot(p1,legend=false)\n\n\nsol = [-0.0, -0.0]\nitérés = [[9.0, 1.0], [7.2, -0.8], [5.760000000000002, 0.6400000000000001], [4.608000000000001, -0.5120000000000001], [3.686400000000001, 0.4096000000000001], [2.9491200000000006, -0.3276800000000001]]\nxsol = [7.2, -0.8]\nflag = 3\nfsol = 28.8\n∇f_xsol = [7.2, -7.2]\nnb_iter = 1\nxsol = [5.76, 0.6400000000000001]\nflag = 3\nfsol = 18.432\n∇f_xsol = [5.76, 5.760000000000002]\nnb_iter = 2\nxsol = [4.6080000000000005, -0.5119999999999998]\nflag = 3\nfsol = 11.79648\n∇f_xsol = [4.6080000000000005, -4.607999999999998]\nnb_iter = 3\nxsol = [3.6864, 0.4096000000000003]\nflag = 3\nfsol = 7.5497472000000005\n∇f_xsol = [3.6864, 3.6864000000000026]\nnb_iter = 4\nxsol = [2.9491200000000006, -0.32767999999999975]\nflag = 3\nfsol = 4.831838208000001\n∇f_xsol = [2.9491200000000006, -2.949119999999998]\nnb_iter = 5\nxsol = [2.359296, 0.26214400000000027]\nflag = 3\nfsol = 3.092376453120001\n∇f_xsol = [2.359296, 2.3592960000000023]\nnb_iter = 6\nxsol = [1.8874368000000004, -0.20971519999999988]\nflag = 3\nfsol = 1.9791209299968004\n∇f_xsol = [1.8874368000000004, -1.8874367999999988]\nnb_iter = 7"
  },
  {
    "objectID": "TP-steepest-descent.html#steepest-descent-for-a-quadratric-function",
    "href": "TP-steepest-descent.html#steepest-descent-for-a-quadratric-function",
    "title": "Gradient Descent",
    "section": "",
    "text": "Code\n\n\"\"\"\n   Solve Min f(x)=0.5x^TAx +b^Tx+c\n   by the steepest descent \n\"\"\"\nfunction steepest_descent_quad(A::Matrix{&lt;:Real},b::Vector{&lt;:Real},c::Real,x0::Vector{&lt;:Real};AbsTol= abs(eps()), RelTol = abs(eps()), ε=0.01, nbit_max = 0)\n  # To complete\n  \n  return xₖ, flag, fₖ, ∇fₖ, k  \nend\n\n\n\n\nCode\nA = [1 0 ; 0 9]\nb = [0,0]; c=0.\nf(x) = 0.5*x'*A*x + b'*x +c\n∇f(x) = 0.5*(A' + A)*x+b\nx = range(-9,stop=9,length=100)\ny = range(-9,stop=9,length=100)\nf_contour(x,y) = f([x,y])\nz = @. f_contour(x', y)\nnb_levels = 7\nlevels = [f(0.8^k*[9,(-1)^k]) for k in nb_levels:-1:0]\np1 = contour(x,y,z,levels=levels,cbar=false,color=:turbo)\nx0 = [9,1]\nprintln(\"itérés = \", [0.8^k*[9,(-1)^k] for k in 0:5])\nxₖ = x0\nfor nbit in 1:nb_levels\n  xsol, flag, fsol, ∇f_xsol , nb_iter  = steepest_descent(A,b,c,x0,nbit_max=nbit)\n  println(\"xsol = \", xsol)\n  println(\"flag = \", flag)\n  println(\"fsol = \", fsol)\n  println(\"∇f_xsol = \", ∇f_xsol)\n  println(\"nb_iter = \", nb_iter)\n  plot!(p1,[xₖ[1],xsol[1]],[xₖ[2],xsol[2]])\nxₖ = xsol\nend\n\nplot(p1,legend=false)\n\n\nsol = [-0.0, -0.0]\nitérés = [[9.0, 1.0], [7.2, -0.8], [5.760000000000002, 0.6400000000000001], [4.608000000000001, -0.5120000000000001], [3.686400000000001, 0.4096000000000001], [2.9491200000000006, -0.3276800000000001]]\nxsol = [7.2, -0.8]\nflag = 3\nfsol = 28.8\n∇f_xsol = [7.2, -7.2]\nnb_iter = 1\nxsol = [5.76, 0.6400000000000001]\nflag = 3\nfsol = 18.432\n∇f_xsol = [5.76, 5.760000000000002]\nnb_iter = 2\nxsol = [4.6080000000000005, -0.5119999999999998]\nflag = 3\nfsol = 11.79648\n∇f_xsol = [4.6080000000000005, -4.607999999999998]\nnb_iter = 3\nxsol = [3.6864, 0.4096000000000003]\nflag = 3\nfsol = 7.5497472000000005\n∇f_xsol = [3.6864, 3.6864000000000026]\nnb_iter = 4\nxsol = [2.9491200000000006, -0.32767999999999975]\nflag = 3\nfsol = 4.831838208000001\n∇f_xsol = [2.9491200000000006, -2.949119999999998]\nnb_iter = 5\nxsol = [2.359296, 0.26214400000000027]\nflag = 3\nfsol = 3.092376453120001\n∇f_xsol = [2.359296, 2.3592960000000023]\nnb_iter = 6\nxsol = [1.8874368000000004, -0.20971519999999988]\nflag = 3\nfsol = 1.9791209299968004\n∇f_xsol = [1.8874368000000004, -1.8874367999999988]\nnb_iter = 7"
  },
  {
    "objectID": "TP-steepest-descent.html#gradient-descent-with-backtracking",
    "href": "TP-steepest-descent.html#gradient-descent-with-backtracking",
    "title": "Gradient Descent",
    "section": "Gradient descent with backtracking",
    "text": "Gradient descent with backtracking\n\n\nCode\nfunction backtracking(g,fₖ,∇fₖ,dₖ;α₀=1.,ρ=0.8,c₁=1.e-4)\n    αₖ = α₀\n\n    while g(αₖ) &gt; fₖ + c₁αₖ∇fₖ'*dₖ\n        αₖ = ρ*αₖ\n    return α\nend"
  },
  {
    "objectID": "NN.html",
    "href": "NN.html",
    "title": "Neurol Network”",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbf{R}}}\n\\newcommand{\\abs}[1]{\\lvert#1\\rvert}\n\\newcommand{\\norme}[1]{\\lVert#1\\rVert}\n\\]",
    "crumbs": [
      "Neurol Network\""
    ]
  },
  {
    "objectID": "NN.html#simple-linear-regression",
    "href": "NN.html#simple-linear-regression",
    "title": "Neurol Network”",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\n\nCode\nx1 = 1.; x2 = sqrt(5*9/2-x1^2);\nx = [-x2, -x1, 0., x1, x2]\n\nn = length(x)\nX = [ones(n)  x]\n\na₁ = 1; a₀ = 2\ny = a₁*x .+ a₀ # model\n\ninclude(\"src/MyOptims.jl\")\nA = X'*X\nprintln(\"A = \", A)\nb = -X'*y\nprintln(\"b = \", b)\nprintln(\"n= \",n)\nprintln(\"X*x0 = \", X*x0)\nf(x) = (2/n)*(0.5*x'*A*x + b'*x + 0.5*y'*y) \ng(β) = (1/n)*norm(X*β-y)^2 \nprintln(\"x0 = \", x0)\nprintln(\"f(x0) = \", f(x0))\nprintln(\"g(x0) = \", g(x0))\nprintln(\"grad = \", (2/n)*(A*x0+b))\nxₖ = x0\n#p1 = plot()\nXsol = zeros(nb_levels+1,2)\nXsol[1,:] = x0\nfor nbit in 1:nb_levels\n  xsol, flag, fsol, ∇f_xsol , nb_iter  = my_descent(f,x0,nbit_max=nbit)\n  println(\"xsol = \", xsol)\n  xₖ = xsol\n  Xsol[nbit+1,:] = xsol\nend\n\n\nA = [5.0 0.0; 0.0 45.0]\nb = [-10.0, -45.0]\nn= 5\nX*x0 = [10.90797688063037, 20.0, 22.5, 25.0, 34.09202311936963]\nx0 = [22.5, 2.5]\nf(x0) = 440.5\ng(x0) = 440.49999999999994\ngrad = [41.0, 27.0]\nxsol = [18.4, -0.20000000000000018]\nxsol = [15.119999999999997, 1.9600000000000004]\nxsol = [12.495999999999997, 0.23199999999999954]\nxsol = [10.396799999999997, 1.6144000000000003]\nxsol = [8.717439999999998, 0.5084799999999998]\nxsol = [7.373951999999998, 1.3932160000000002]\nxsol = [6.299161599999998, 0.6854271999999997]\n\n\n\n\nCode\nx_train = reshape(Float32.(x),1,n)\nprintln(\"x_train = \", x_train)\ny_train = reshape(Float32.(y),1,n)\nprintln(\"y_train = \", y_train)\nusing Flux, Statistics\nmodel = Dense(1 =&gt; 1)\nmodel.weight[1,1]=Float32((n/2))\nmodel.bias[1] = Float32((n/2)*9)\nprintln(\"model.weight = \", model.weight)\nprintln(\"model.bias = \", model.bias)\nprintln(\"model(x_train) = \", model(x_train))\nloss(model, x, y) = mean(abs2.(model(x) .- y));\nprintln(\"loss(model, x_train, y_train) = \", loss(model, x_train, y_train))\nprintln(abs2.(model(x_train) .- y_train))\n\nopt = Descent()\ndata = [(x_train, y_train)]\nprintln(\"data = \", data)\nprintln(\"n= \",n)\n#data = Flux.DataLoader((x_train, y_train), batchsize=5)\nprintln(\"data = \", first(data))\nnb_levels = 7 \nx0 = (n/2)*[9,1]\nXsolNN = zeros(nb_levels+1,2)\nXsolNN[1,:] = x0\nprintln(\"Flux.setup(rule, model) = \", Flux.setup(opt, model))\ndLdm, _, _ = gradient(loss, model, x_train, y_train)\nprintln(\"dLdm = \", dLdm)\nfor epoch in 1:nb_levels\n    Flux.train!(loss, model, data, opt)\n    println(\"model.weight = \", model.weight)\nprintln(\"model.bias = \", model.bias)\n    XsolNN[epoch+1,:] = [model.bias[1], model.weight[1,1]]\nend\n\n\n\nprintln(\"Xsol = \", Xsol)\nprintln(\"XsolNN = \", XsolNN)\nprintln(\"Xsol - XsolNN\")\ndisplay(Xsol - XsolNN)\n\n\nx_train = Float32[-4.6368093 -1.0 0.0 1.0 4.6368093]\ny_train = Float32[-2.6368093 1.0 2.0 3.0 6.6368093]\nmodel.weight = Float32[2.5;;]\nmodel.bias = Float32[22.5]\nmodel(x_train) = Float32[10.907976 20.0 22.5 25.0 34.092026]\nloss(model, x_train, y_train) = 440.5\nFloat32[183.46121 361.0 420.25 484.0 753.7889]\ndata = Tuple{Matrix{Float32}, Matrix{Float32}}[([-4.6368093 -1.0 0.0 1.0 4.6368093], [-2.6368093 1.0 2.0 3.0 6.6368093])]\nn= 5\ndata = (Float32[-4.6368093 -1.0 0.0 1.0 4.6368093], Float32[-2.6368093 1.0 2.0 3.0 6.6368093])\nFlux.setup(rule, model) = (weight = Leaf(Descent(0.1), nothing), bias = Leaf(Descent(0.1), nothing), σ = ())\ndLdm = (weight = Float32[27.000004;;], bias = Float32[41.0], σ = nothing)\nmodel.weight = Float32[-0.20000052;;]\nmodel.bias = Float32[18.4]\nmodel.weight = Float32[1.9600008;;]\nmodel.bias = Float32[15.12]\nmodel.weight = Float32[0.23199975;;]\nmodel.bias = Float32[12.496]\nmodel.weight = Float32[1.6144003;;]\nmodel.bias = Float32[10.3968]\nmodel.weight = Float32[0.50847995;;]\nmodel.bias = Float32[8.717441]\nmodel.weight = Float32[1.3932161;;]\nmodel.bias = Float32[7.3739524]\nmodel.weight = Float32[0.685427;;]\nmodel.bias = Float32[6.299162]\nXsol = [22.5 2.5; 18.4 -0.20000000000000018; 15.119999999999997 1.9600000000000004; 12.495999999999997 0.23199999999999954; 10.396799999999997 1.6144000000000003; 8.717439999999998 0.5084799999999998; 7.373951999999998 1.3932160000000002; 6.299161599999998 0.6854271999999997]\nXsolNN = [22.5 2.5; 18.399999618530273 -0.20000052452087402; 15.119999885559082 1.96000075340271; 12.496000289916992 0.23199975490570068; 10.39680004119873 1.6144002676010132; 8.717440605163574 0.5084799528121948; 7.373952388763428 1.3932161331176758; 6.299161911010742 0.6854270100593567]\nXsol - XsolNN\n\n\n8×2 Matrix{Float64}:\n  0.0          0.0\n  3.8147e-7    5.24521e-7\n  1.14441e-7  -7.53403e-7\n -2.89917e-7   2.45094e-7\n -4.11987e-8  -2.67601e-7\n -6.05164e-7   4.71878e-8\n -3.88763e-7  -1.33118e-7\n -3.11011e-7   1.89941e-7",
    "crumbs": [
      "Neurol Network\""
    ]
  },
  {
    "objectID": "NN.html#descent-algorithm-with-constant-rate",
    "href": "NN.html#descent-algorithm-with-constant-rate",
    "title": "Neurol Network”",
    "section": "Descent algorithm with constant rate",
    "text": "Descent algorithm with constant rate\n\nAlgorithm\nRequire \\(f \\colon \\R^n \\to \\R\\), \\(x_{0} \\in \\R^n\\) ( initial point)\n\n\\(\\eta = 0.1\\)\n\\(k \\gets 0\\)\ncontinue = true\nWhile continue\n\n\\(d_k = -\\nabla f(x_k)\\)\n\\(x_{k+1} \\gets x_{k} + \\eta d_{k}\\)\n\n\\(k \\gets k + 1\\)\ncontinue = stop_function(\\(\\nabla f_k\\),\\(x_k\\),\\(x_{k+1}\\),\\(f_k\\),\\(f_{k+1}\\),AbsTol,RelTol,\\(\\varepsilon\\))\n\nEndWhile\n\n\n\nApplication : Simple Linear Regression\n\n\nCode\nx1 = 1.; x2 = sqrt(5*9/2-x1^2);\nx = [-x2, -x1, 0., x1, x2]\n\nn = length(x)\nX = [ones(n)  x]\n\na₁ = 1; a₀ = 2\ny = a₁*x .+ a₀ # model\n\ninclude(\"src/MyOptims.jl\")\nA = X'*X\nprintln(\"A = \", A)\nb = -X'*y\nprintln(\"b = \", b)\nprintln(\"n= \",n)\nx0 = (n/2)*[9,1]\nprintln(\"X*x0 = \", X*x0)\nf(x) = (2/n)*(0.5*x'*A*x + b'*x + 0.5*y'*y) \n\nprintln(\"x0 = \", x0)\nprintln(\"f(x0) = \", f(x0))\nxₖ = x0\n#p1 = plot()\nnb_levels = 7\nXsol = zeros(nb_levels+1,2)\nXsol[1,:] = x0\nfor nbit in 1:nb_levels\n  xsol, flag, fsol, ∇f_xsol , nb_iter  = my_descent(f,x0,nbit_max=nbit)\n  println(\"xsol = \", xsol)\n  xₖ = xsol\n  Xsol[nbit+1,:] = xsol\nend\n\n\nA = [5.0 0.0; 0.0 45.0]\nb = [-10.0, -45.0]\nn= 5\nX*x0 = [10.90797688063037, 20.0, 22.5, 25.0, 34.09202311936963]\nx0 = [22.5, 2.5]\nf(x0) = 440.5\nxsol = [18.4, -0.20000000000000018]\nxsol = [15.119999999999997, 1.9600000000000004]\nxsol = [12.495999999999997, 0.23199999999999954]\nxsol = [10.396799999999997, 1.6144000000000003]\nxsol = [8.717439999999998, 0.5084799999999998]\nxsol = [7.373951999999998, 1.3932160000000002]\nxsol = [6.299161599999998, 0.6854271999999997]",
    "crumbs": [
      "Neurol Network\""
    ]
  },
  {
    "objectID": "NN.html#with-the-flux-package",
    "href": "NN.html#with-the-flux-package",
    "title": "Neurol Network”",
    "section": "With the Flux Package",
    "text": "With the Flux Package\nFind the same results with Flux (see https://fluxml.ai/Flux.jl/stable/)\n\n\nCode\nx_train = reshape(Float32.(x),1,n)\nprintln(\"x_train = \", x_train)\ny_train = reshape(Float32.(y),1,n)\nprintln(\"y_train = \", y_train)\nusing Flux, Statistics\nmodel = Dense(1 =&gt; 1)\nmodel.weight[1,1]=Float32((n/2))\nmodel.bias[1] = Float32((n/2)*9)\nprintln(\"model.weight = \", model.weight)\nprintln(\"model.bias = \", model.bias)\nprintln(\"model(x_train) = \", model(x_train))\nloss(model, x, y) = mean(abs2.(model(x) .- y));\nprintln(\"loss(model, x_train, y_train) = \", loss(model, x_train, y_train))\nprintln(abs2.(model(x_train) .- y_train))\n\nopt = Descent()\ndata = [(x_train, y_train)]\nprintln(\"data = \", data)\nprintln(\"n= \",n)\n#data = Flux.DataLoader((x_train, y_train), batchsize=5)\nprintln(\"data = \", first(data))\n\nXsolNN = zeros(nb_levels+1,2)\nXsolNN[1,:] = x0\nprintln(\"Flux.setup(rule, model) = \", Flux.setup(opt, model))\ndLdm, _, _ = gradient(loss, model, x_train, y_train)\nprintln(\"dLdm = \", dLdm)\nfor epoch in 1:nb_levels\n    Flux.train!(loss, model, data, opt)\n    println(\"model.weight = \", model.weight)\n    println(\"model.bias = \", model.bias)\n    XsolNN[epoch+1,:] = [model.bias[1], model.weight[1,1]]\nend\n\nprintln(\"Xsol = \", Xsol)\nprintln(\"XsolNN = \", XsolNN)\nprintln(\"Xsol - XsolNN\")\ndisplay(Xsol - XsolNN)\n\n\nx_train = Float32[-4.6368093 -1.0 0.0 1.0 4.6368093]\ny_train = Float32[-2.6368093 1.0 2.0 3.0 6.6368093]\nmodel.weight = Float32[2.5;;]\nmodel.bias = Float32[22.5]\nmodel(x_train) = Float32[10.907976 20.0 22.5 25.0 34.092026]\nloss(model, x_train, y_train) = 440.5\nFloat32[183.46121 361.0 420.25 484.0 753.7889]\ndata = Tuple{Matrix{Float32}, Matrix{Float32}}[([-4.6368093 -1.0 0.0 1.0 4.6368093], [-2.6368093 1.0 2.0 3.0 6.6368093])]\nn= 5\ndata = (Float32[-4.6368093 -1.0 0.0 1.0 4.6368093], Float32[-2.6368093 1.0 2.0 3.0 6.6368093])\nFlux.setup(rule, model) = (weight = Leaf(Descent(0.1), nothing), bias = Leaf(Descent(0.1), nothing), σ = ())\ndLdm = (weight = Float32[27.000004;;], bias = Float32[41.0], σ = nothing)\nmodel.weight = Float32[-0.20000052;;]\nmodel.bias = Float32[18.4]\nmodel.weight = Float32[1.9600008;;]\nmodel.bias = Float32[15.12]\nmodel.weight = Float32[0.23199975;;]\nmodel.bias = Float32[12.496]\nmodel.weight = Float32[1.6144003;;]\nmodel.bias = Float32[10.3968]\nmodel.weight = Float32[0.50847995;;]\nmodel.bias = Float32[8.717441]\nmodel.weight = Float32[1.3932161;;]\nmodel.bias = Float32[7.3739524]\nmodel.weight = Float32[0.685427;;]\nmodel.bias = Float32[6.299162]\nXsol = [22.5 2.5; 18.4 -0.20000000000000018; 15.119999999999997 1.9600000000000004; 12.495999999999997 0.23199999999999954; 10.396799999999997 1.6144000000000003; 8.717439999999998 0.5084799999999998; 7.373951999999998 1.3932160000000002; 6.299161599999998 0.6854271999999997]\nXsolNN = [22.5 2.5; 18.399999618530273 -0.20000052452087402; 15.119999885559082 1.96000075340271; 12.496000289916992 0.23199975490570068; 10.39680004119873 1.6144002676010132; 8.717440605163574 0.5084799528121948; 7.373952388763428 1.3932161331176758; 6.299161911010742 0.6854270100593567]\nXsol - XsolNN\n\n\n8×2 Matrix{Float64}:\n  0.0          0.0\n  3.8147e-7    5.24521e-7\n  1.14441e-7  -7.53403e-7\n -2.89917e-7   2.45094e-7\n -4.11987e-8  -2.67601e-7\n -6.05164e-7   4.71878e-8\n -3.88763e-7  -1.33118e-7\n -3.11011e-7   1.89941e-7",
    "crumbs": [
      "Neurol Network\""
    ]
  },
  {
    "objectID": "least-squares-pb.html",
    "href": "least-squares-pb.html",
    "title": "Least Squares Probem",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbf{R}}}\n\\newcommand{\\abs}[1]{\\lvert#1\\rvert}\n\\newcommand{\\norme}[1]{\\lVert#1\\rVert}\n\\]\n\n\nLeast Squares Probem\n\n\n\n\n\n\nExample : $C^{14} Datation\n\n\n\nRadioactive carbon \\(^{14}C\\) is produced in the atmosphere by the effect of cosmic rays on atmospheric nitrogen. It is oxidized to \\(^{14}CO_{2}\\) and absorbed in this form by living organisms. So, living organisms contain a certain percentage of radioactive carbon relative to \\(^{12}C\\) and \\(^{13}C\\) which are stable. We suppose that carbon production \\(^{14}C\\) is constant over the last few millennia.\nIt is also assumed that, when an organism dies, its exchanges with the atmosphere cease, and that radioactivity due to carbon to carbon \\(^{14}C\\) decreases according to the following exponential law:\n\\[\nA(t,A_0,\\lambda)=A_{0}e^{-\\lambda t}.\n\\]\nThe analysis of the trunks (wood is dead tissue) of old trees {} and {} furnishes us~:\n\nits age \\(t\\) in year\nits radioactivity \\(A\\)\n\n\\[\n\\begin{array}{||c|ccccccc||}\\hline\\hline\nt_i & 500 & 1000 & 2000 & 3000 & 4000 & 5000 & 6300 \\\\ \\hline\nA_i & 14.5 & 13.5 & 12.0 & 10.8 & 9.9 & 8.9 & 8.0\\\\ \\hline\\hline\n\\end{array}\n\\]\nWe want to find the values of the parameters \\(A_0\\) and \\(\\lambda\\), so that the function \\(A{(t,A_0,\\lambda)}\\) is “near” the data :\n\\[(P) \\left\\{   \n  \\begin{array}{l}\n    \\displaystyle Min\\; f(\\beta) = \\frac{1}{2}\\|r(\\beta\\|^2 = f(A_0,\\lambda)=\\frac{1}{2}\\sum_{i=1}^{n}(A_i-A_0e^{-\\lambda t_i})^2\\\\\n    \\beta=(A_0,\\lambda) \\in  {\\R}^2.\n  \\end{array} \\right.\n  \\]\nSolve this problem by using :\n\nThe Newton algorithm\nThe Gauss-Newton algorithm :\n\n\\[({P}_k)\\left\\{\\begin{array}{l}\n  Min\\;\\;f_k(s)=\\frac{1}{2}\\|r(\\beta^{(k)})+J_r(\\beta^{(k)})s\\|^2\\\\\n  s\\in \\R^p,\n\\end{array}\\right.\\] where \\(s = \\beta - \\beta^{(k)}\\) abd \\(J_r(\\beta)\\) is the Jacobian matrix of \\(r\\) in \\(\\beta\\).\n\nData, Residuals and \\(f\\) functions\n\n\nCode\nusing LinearAlgebra\nusing Plots\nTi = [ 500, 1000, 2000, 3000, 4000, 5000, 6300];\nAi = [14.5, 13.5, 12.0, 10.8,  9.9,  8.9,  8.0];\nn = length(Ti)\nData_C14 = [Ti Ai];\nprintln(Data_C14)\n\n# Initialisation\nbeta0 = [10; 0.0001];   # Newton, Gauus-Newton, fminunc et leastsq converge\n# beta0 = [15; 0.001];    # Newton, Gauss-Newton, fminunc et leastsq divergent\n# beta0 = [15; 0.0005];   # Newton diverge, Gauss-Newton, fminunc et leastsq convergent\n# beta0 = [10; 0.0005];   # Gauss-Newton converge\n\n# Initial model\n#----------------------------------\nxmin = 9; xmax = 20;\nxx = range(xmin, stop=xmax, length=100);\n\nymin = -0.0001; ymax = 0.0005;\nyy = range(ymin, stop=ymax, length=100);\n\n# Residual function\nfunction r(β,data)\n    A₀ = β[1]\n    λ = β[2]\n    return data[:,2]-A₀*exp.(-λ*data[:,1])\nend\n# Plot of the function\n\nprintln(\"r(beta0,Data_C14) = \", r(beta0,Data_C14))\nX = [ones(n) Ti]\nf(β) = 0.5*norm(r(β,Data_C14))^2\nf_contour(A₀,λ) = f([A₀,λ])\nz = @. f_contour(xx', yy)\n\np1 = plot()\ncontour!(p1,xx,yy,z,levels=100,cbar=false,color=:turbo)\n\np2 = scatter(Ti,Ai,title=\"Data C14\")\nT = range(0,stop=6500,length=100);\nA = beta0[1]*exp.(-beta0[2]*T);\nplot!(p2,T,A)\n\n\n[500.0 14.5; 1000.0 13.5; 2000.0 12.0; 3000.0 10.8; 4000.0 9.9; 5000.0 8.9; 6300.0 8.0]\nr(beta0,Data_C14) = [4.987705754992859, 4.451625819640405, 3.812692469220181, 3.391817793182822, 3.1967995396436066, 2.8346934028736666, 2.674081989931028]\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith the Gauss-Newton Algorithm\n\n\nCode\n# solve by Gauss-Newton Algorithm\nusing Plots\ninclude(\"src/MyOptims.jl\")\nnb_levels = 5\nres(β) = r(β,Data_C14)\nfor nbit in 1:nb_levels\n    βsol, flag, fsol, ∇f_xsol , nb_iter  = algo_Gauss_Newton(res,beta0,nbit_max=nbit)\n #   A = βsol[1]*exp.(-βsol[2]*T);\n    plot!(p2,T,βsol[1]*exp.(-βsol[2]*T))\n    scatter!(p1,[βsol[1]],[βsol[2]])\nend\n\nplot(p1,p2,legend=false)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith Newton Algorithm\n\n\nCode\n# solve by Newton Algorithm\n\nnb_levels = 5\nfor nbit in 1:nb_levels\n    βsol, flag, fsol, ∇f_xsol , nb_iter  = algo_Newton(f,beta0,nbit_max=nbit)\n #   A = βsol[1]*exp.(-βsol[2]*T);\n    plot!(p2,T,βsol[1]*exp.(-βsol[2]*T))\n    scatter!(p1,[βsol[1]],[βsol[2]])\nend\n\nplot(p1,p2,legend=false)",
    "crumbs": [
      "Least Squares Probem"
    ]
  },
  {
    "objectID": "NN.html#mnist",
    "href": "NN.html#mnist",
    "title": "Neurol Network”",
    "section": "MNIST",
    "text": "MNIST\nThis example comes from the book Statistics with julia, Yoni Nazarathy and Hayden Klol, Springer, 2021",
    "crumbs": [
      "Neurol Network\""
    ]
  },
  {
    "objectID": "TP-introduction-NN.html",
    "href": "TP-introduction-NN.html",
    "title": "Neural Network",
    "section": "",
    "text": "Complete the descent with constant rate function\nCode\nfunction my_descent(f,x0::Vector{&lt;:Real};η=0.1,AbsTol= abs(eps()), RelTol = abs(eps()), ε=0.01, nbit_max = 0)# to complete\n\n  return xₖ, flag, fₖ, ∇fₖ, k  c\nend\nAnd apply this code for solving the following problem\n\\[(P)\\left\\{\\begin{array}{l}\nMin\\;f(\\beta) = \\frac{1}{n}\\|X\\beta - y\\|^2\\\\\n\\beta\\in\\R^2\n\\end{array}\\right.\n\\] with \\[X = \\begin{pmatrix}\n1 & x_1\\\\\n\\vdots & \\vdots\\\\\n1 & x_5\n\\end{pmatrix}\n\\;\\;\\textrm{and}\\;\\;\ny = \\begin{pmatrix}\ny_1\\\\\n\\vdots\\\\\ny_5\n\\end{pmatrix}\n\\]\nYou ’ll print the first 7 iterations for the initual guess \\(x0= (n/2)(9,1)\\).\nCode\nusing Plots\n# n = 5\nx1 = 1.; x2 = sqrt(5*9/2-x1^2);\nx = [-x2,-x1,0.,x1,x2]\nn = length(x)\na₁ = 1; a₀ = 2\ny = a₁*x .+ a₀ # model"
  },
  {
    "objectID": "TP-introduction-NN.html#section",
    "href": "TP-introduction-NN.html#section",
    "title": "Neural Network",
    "section": "",
    "text": "Code\nfunction descent_backtrac(f,x0::Vector{&lt;:Real};AbsTol= abs(eps()), RelTol = abs(eps()), ε=0.01, nbit_max = 0)\n    # to complete\n\n  return xₖ, flag, fₖ, ∇fₖ, k  c\nend\n\n\n\n\nCode\nusing Plots\nx = range(-10*(n/2)-xsol[1],stop=10*(n/2)-xsol[1],length=100)\ny = range(-9*(n/2)-xsol[2],stop=9*(n/2)-xsol[2],length=100)\nf_contour(x,y) = f([x,y])\nz = @. f_contour(x', y)\nnb_levels = 7\nx0 = (n/2)*[9,1]"
  },
  {
    "objectID": "src/ttt.html",
    "href": "src/ttt.html",
    "title": "Cours",
    "section": "",
    "text": "Code\nfunction tata()\n    a=1\n    for i in 1:2\n        a= 0\n    end\n    return a\nend\n\ntata()\n\n\nUndefVarError: UndefVarError: `a` not defined\nUndefVarError: `a` not defined\n\n\n\nStacktrace:\n\n [1] tata()\n\n   @ Main ~/git-ENS/Julia-TSE/enseignants/M2/Cours/src/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W0sZmlsZQ==.jl:5\n\n [2] top-level scope\n\n   @ ~/git-ENS/Julia-TSE/enseignants/M2/Cours/src/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W0sZmlsZQ==.jl:8"
  }
]