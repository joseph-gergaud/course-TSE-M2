[
  {
    "objectID": "PCA.html",
    "href": "PCA.html",
    "title": "Principal component analysis (PCA)",
    "section": "",
    "text": "Principal component analysis (PCA) reduces the number of dimensions in large datasets to principal components that retain most of the original information. It does this by transforming potentially correlated variables into a smaller set of uncorrelated variables, called principal components.\nLet \\(X\\) the matrix \\((n,p)\\) of data, we note \\(X_c\\) = the centered matrix. Then the empirical variances, covariances matrix is \\(C = \\frac{1}{n}X_c^TX_c\\). We note \\(\\Lambda\\) the vector of the eigen value (in decrease order) of the matrix \\(C\\) and \\(U\\) the \\((p,p)\\) orthogonal matrix of the eigen vectors : \\[C=U\\texttt{diag}(\\Lambda) U^T.\\] Then We have\n\nThe coordinates of the \\(n\\) observations in the new basis of the eigen vectors \\((\\vec{u}_1,\\ldots,\\vec{u}_p)\\) are \\[\\Psi = X_cU\\]\nThe The coordinates of the \\(p\\) variables in the new basis of the eigen vectors \\((\\vec{v}_1,\\ldots,\\vec{v}_p)\\) are \\[\\Phi = \\sqrt{n}U\\texttt{diag}(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda}_p)\\]\nThe total inertia (variance) is \\[I = \\texttt{trace}(C)=\\sum_{i=1,p}\\lambda_i\\]\nThe variance of the variable \\(v_i\\) is \\(\\lambda_i\\)",
    "crumbs": [
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "PCA.html#introduction",
    "href": "PCA.html#introduction",
    "title": "Principal component analysis (PCA)",
    "section": "",
    "text": "Principal component analysis (PCA) reduces the number of dimensions in large datasets to principal components that retain most of the original information. It does this by transforming potentially correlated variables into a smaller set of uncorrelated variables, called principal components.\nLet \\(X\\) the matrix \\((n,p)\\) of data, we note \\(X_c\\) = the centered matrix. Then the empirical variances, covariances matrix is \\(C = \\frac{1}{n}X_c^TX_c\\). We note \\(\\Lambda\\) the vector of the eigen value (in decrease order) of the matrix \\(C\\) and \\(U\\) the \\((p,p)\\) orthogonal matrix of the eigen vectors : \\[C=U\\texttt{diag}(\\Lambda) U^T.\\] Then We have\n\nThe coordinates of the \\(n\\) observations in the new basis of the eigen vectors \\((\\vec{u}_1,\\ldots,\\vec{u}_p)\\) are \\[\\Psi = X_cU\\]\nThe The coordinates of the \\(p\\) variables in the new basis of the eigen vectors \\((\\vec{v}_1,\\ldots,\\vec{v}_p)\\) are \\[\\Phi = \\sqrt{n}U\\texttt{diag}(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda}_p)\\]\nThe total inertia (variance) is \\[I = \\texttt{trace}(C)=\\sum_{i=1,p}\\lambda_i\\]\nThe variance of the variable \\(v_i\\) is \\(\\lambda_i\\)",
    "crumbs": [
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "PCA.html#iris-data",
    "href": "PCA.html#iris-data",
    "title": "Principal component analysis (PCA)",
    "section": "Iris Data",
    "text": "Iris Data\n\n\nCode\nusing Plots\nusing Statistics, LinearAlgebra\nusing RDatasets, DataFrames\niris = RDatasets.dataset(\"datasets\", \"iris\")  # Iris Datas\nNames = names(iris)\nX = Matrix(iris[:,1:4])\np1 = scatter(X[:,1],X[:,2], c=[:blue :red :green], group=iris.Species,xlabel = Names[1],ylabel=Names[2])\np2 = scatter(X[:,3],X[:,4], c=[:blue :red :green], group=iris.Species,xlabel = Names[3],ylabel=Names[4])\nplot(p1,p2)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy PCA function\n\n\nCode\nusing LinearAlgebra, Statistics\nfunction my_PCA(X::Matrix{&lt;:Real};normed=false)::Tuple{Vector{&lt;:Real},Matrix{&lt;:Real},Matrix{&lt;:Real},Matrix{&lt;:Real},Real,Vector{&lt;:Real},Matrix{&lt;:Real}}\n\"\"\"\n    Compute the PCA of Data\n    Input\n    X : (n,p) Matrix of reals\n         n = number of observations\n         p = number of variables\n    Output\n        Λ : Vector of the p eigen value in decrease order\n        U : (p,p) Matrix of reals\n            eigen vectors in column\n        Ψ : (n,p) Matrix of reals\n            Coordinates of the observation in the new basis\n        Φ = (p,p) Matrix of reals\n             Coordinates of the variables in the new basis\n        I_total : Real\n             total inertia\n        cum_var_ratio : p vector of reals\n             cumulative variance ratio\n\"\"\"\n     n,p = size(X)\n     Λ = zeros(p); U = zeros(p,p); Ψ = zeros(n,p); Φ = zeros(p,p); I_total=0; cum_var_ratio = zeros(p)\n     # Calculation of centered data\n     xbar = mean(X,dims=1)\n     Xc = X - ones(n,1)*xbar\n     covMat = (1/n)*Xc'*Xc\n     if normed == true\n     s=std(Xc,corrected=false,dims=1)\n     Y=(Xc)./(ones(n,1)*s);\n     covMat=(1/n)*Y'*Y\n     end\n\n\n     # Computating total inertia\n     I_total = tr(covMat)\n     Λ, U = eigen(covMat)\n     eigOrder = sortperm(Λ, rev = true) # for abtaining increase order of eigen values\n     Λ = Λ[eigOrder]\n     # cumulative variance ratios\n     cum_var_ratio =  Vector{Float64}(undef,p)\n     for i in 1:p\n         cum_var_ratio[i] = sum(Λ[1:i])/I_total\n     end\n     U = U[:,eigOrder]\n     if normed == true\n       Ψ = Y*U\n       Φ = U*sqrt.(diagm(Λ))\n     else\n       Ψ = Xc*U\n       Φ = U*sqrt.(n*diagm(Λ))\n     end\n     return Λ, U, Ψ, Φ, I_total,cum_var_ratio, covMat\nend\n\n\nmy_PCA (generic function with 1 method)\n\n\n\n\nPrint results\n\n\nCode\nmy_PCA_results = my_PCA(X)\nmy_Λ, my_U, my_Ψ, my_Φ, my_I_total, my_cum_var_ratio, my_cov_mat = my_PCA_results\nprintln(\"lambda = \", my_Λ)\nprintln(\"my_U = \")\ndisplay(my_U)\nprintln(\"my_Ψ = \")\ndisplay(my_Ψ)\nprintln(\"my_Φ = \")\ndisplay(my_Φ)\nprintln(\"Total inertia = \", my_I_total)\nprintln(\"my_cum_var_ratio = \", my_cum_var_ratio)\nprintln(\"Matrix of variance, covariance = \")\ndisplay(my_cov_mat)\n\n\nlambda = [4.200053427994632, 0.24105294294244242, 0.07768810337596654, 0.023676192353626366]\nmy_U = \n\n\n4×4 Matrix{Float64}:\n -0.361387    0.656589   0.58203     0.315487\n  0.0845225   0.730161  -0.597911   -0.319723\n -0.856671   -0.173373  -0.0762361  -0.479839\n -0.358289   -0.075481  -0.545831    0.753657\n\n\nmy_Ψ = \n\n\n150×4 Matrix{Float64}:\n  2.68413   0.319397    0.0279148   0.00226244\n  2.71414  -0.177001    0.210464    0.0990266\n  2.88899  -0.144949   -0.0179003   0.0199684\n  2.74534  -0.318299   -0.0315594  -0.0755758\n  2.72872   0.326755   -0.0900792  -0.0612586\n  2.28086   0.74133    -0.168678   -0.0242009\n  2.82054  -0.0894614  -0.257892   -0.0481431\n  2.62614   0.163385    0.0218793  -0.0452979\n  2.88638  -0.578312   -0.0207596  -0.0267447\n  2.67276  -0.113774    0.197633   -0.0562954\n  2.50695   0.645069    0.075318   -0.0150199\n  2.61276   0.0147299  -0.10215    -0.156379\n  2.78611  -0.235112    0.206844   -0.00788791\n  ⋮                                \n -1.16933  -0.16499    -0.281836    0.0204618\n -2.10761   0.372288   -0.0272911   0.210622\n -2.31415   0.183651   -0.322694    0.277654\n -1.92227   0.409203   -0.113587    0.505305\n -1.41524  -0.574916   -0.296323   -0.0153047\n -2.56301   0.277863   -0.29257     0.0579127\n -2.41875   0.304798   -0.504483    0.241091\n -1.94411   0.187532   -0.177825    0.426196\n -1.52717  -0.375317    0.121898    0.254367\n -1.76435   0.0788589  -0.130482    0.137001\n -1.90094   0.116628   -0.723252    0.0445953\n -1.39019  -0.282661   -0.36291    -0.155039\n\n\nmy_Φ = \n\n\n4×4 Matrix{Float64}:\n  -9.07079   3.94817    1.98686    0.594543\n   2.12151   4.39057   -2.04108   -0.602526\n -21.5024   -1.04252   -0.260246  -0.904268\n  -8.99304  -0.453878  -1.86329    1.42029\n\n\nTotal inertia = 4.54247066666667\nmy_cum_var_ratio = [0.9246187232017266, 0.9776852063187943, 0.9947878161267241, 0.9999999999999994]\nMatrix of variance, covariance = \n\n\n4×4 Matrix{Float64}:\n  0.681122   -0.0421511   1.26582    0.512829\n -0.0421511   0.188713   -0.327459  -0.120828\n  1.26582    -0.327459    3.0955     1.28697\n  0.512829   -0.120828    1.28697    0.577133\n\n\n\n\nGraph of the observations\n\n\nCode\np3 = scatter(my_Ψ[:,1],my_Ψ[:,2], c=[:blue :red :green], group=iris.Species,xlabel = \"PC1\", ylabel=\"PC2\")\np4 = scatter(my_Ψ[:,3],my_Ψ[:,4], c=[:blue :red :green], group=iris.Species,xlabel = \"PC1\", ylabel=\"PC2\")\nplot(p3,p4)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraph of the variables\n\n\nCode\npvar1 = plot()\nech = 1.1*maximum(abs.(my_Φ))\nfor i=1:4\n    plot!(pvar1,[0,my_Φ[i,1]], [0,my_Φ[i,2]], xlims=(-ech,ech), ylims=(-ech,ech), arrow=true, label=Names[i], legend=:bottomleft, xlabel=\"v1\", ylabel=\"v2\")\nend\n\npvar2 = plot()\nfor i=1:4\n    plot!(pvar2,[0,my_Φ[i,3]], [0,my_Φ[i,4]], xlims=(-ech,ech), ylims=(-ech,ech), arrow=true, label=Names[i], legend=:bottomleft, xlabel=\"v3\", ylabel=\"v4\")\nend\nplot(pvar1,pvar2)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith MultivariateStats package\n\n\nCode\nusing MultivariateStats\n\nmodel = fit(PCA, X', maxoutdim=4, pratio = 0.999)  # Each column of X is an observation\nU = projection(model)\nprintln(\"U = \")\ndisplay(U)\nΨ = MultivariateStats.transform(model, X')\nprintln(\"Ψ = \")\ndisplay(Ψ)\ndisplay(Ψ'-my_Ψ) # Each column of Ψ is an observation\ndisplay(U-my_U)\n\n\n┌ Error: Error during loading of extension SpecialFunctionsExt of ColorVectorSpace, use `Base.retry_load_extensions()` to retry.\n│   exception =\n│    1-element ExceptionStack:\n│    ArgumentError: Package SpecialFunctionsExt [997ecda8-951a-5f50-90ea-61382e97704b] is required but does not seem to be installed:\n│     - Run `Pkg.instantiate()` to install all recorded dependencies.\n│    \n│    Stacktrace:\n│      [1] _require(pkg::Base.PkgId, env::Nothing)\n│        @ Base ./loading.jl:2438\n│      [2] __require_prelocked(uuidkey::Base.PkgId, env::Nothing)\n│        @ Base ./loading.jl:2315\n│      [3] #invoke_in_world#3\n│        @ ./essentials.jl:1089 [inlined]\n│      [4] invoke_in_world\n│        @ ./essentials.jl:1086 [inlined]\n│      [5] _require_prelocked\n│        @ ./loading.jl:2302 [inlined]\n│      [6] _require_prelocked\n│        @ ./loading.jl:2301 [inlined]\n│      [7] run_extension_callbacks(extid::Base.ExtensionId)\n│        @ Base ./loading.jl:1500\n│      [8] run_extension_callbacks(pkgid::Base.PkgId)\n│        @ Base ./loading.jl:1535\n│      [9] run_package_callbacks(modkey::Base.PkgId)\n│        @ Base ./loading.jl:1354\n│     [10] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128, stalecheck::Bool; reasons::Dict{String, Int64}, DEPOT_PATH::Vector{String})\n│        @ Base ./loading.jl:1993\n│     [11] _require(pkg::Base.PkgId, env::String)\n│        @ Base ./loading.jl:2450\n│     [12] __require_prelocked(uuidkey::Base.PkgId, env::String)\n│        @ Base ./loading.jl:2315\n│     [13] #invoke_in_world#3\n│        @ ./essentials.jl:1089 [inlined]\n│     [14] invoke_in_world\n│        @ ./essentials.jl:1086 [inlined]\n│     [15] _require_prelocked(uuidkey::Base.PkgId, env::String)\n│        @ Base ./loading.jl:2302\n│     [16] macro expansion\n│        @ ./loading.jl:2241 [inlined]\n│     [17] macro expansion\n│        @ ./lock.jl:273 [inlined]\n│     [18] __require(into::Module, mod::Symbol)\n│        @ Base ./loading.jl:2198\n│     [19] #invoke_in_world#3\n│        @ ./essentials.jl:1089 [inlined]\n│     [20] invoke_in_world\n│        @ ./essentials.jl:1086 [inlined]\n│     [21] require(into::Module, mod::Symbol)\n│        @ Base ./loading.jl:2191\n│     [22] eval\n│        @ ./boot.jl:430 [inlined]\n│     [23] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n│        @ Base ./loading.jl:2643\n│     [24] softscope_include_string(m::Module, code::String, filename::String)\n│        @ SoftGlobalScope ~/.julia/packages/SoftGlobalScope/u4UzH/src/SoftGlobalScope.jl:65\n│     [25] execute_request(socket::ZMQ.Socket, msg::IJulia.Msg)\n│        @ IJulia ~/.julia/packages/IJulia/dR0lE/src/execute_request.jl:74\n│     [26] #invokelatest#2\n│        @ ./essentials.jl:1055 [inlined]\n│     [27] invokelatest\n│        @ ./essentials.jl:1052 [inlined]\n│     [28] eventloop(socket::ZMQ.Socket)\n│        @ IJulia ~/.julia/packages/IJulia/dR0lE/src/eventloop.jl:8\n│     [29] (::IJulia.var\"#15#18\")()\n│        @ IJulia ~/.julia/packages/IJulia/dR0lE/src/eventloop.jl:38\n└ @ Base loading.jl:1506\n\n\nU = \n\n\n4×4 Matrix{Float64}:\n -0.361387    0.656589   0.58203     0.315487\n  0.0845225   0.730161  -0.597911   -0.319723\n -0.856671   -0.173373  -0.0762361  -0.479839\n -0.358289   -0.075481  -0.545831    0.753657\n\n\nΨ = \n\n\n4×150 Matrix{Float64}:\n 2.68413      2.71414     2.88899    …  -1.76435    -1.90094    -1.39019\n 0.319397    -0.177001   -0.144949       0.0788589   0.116628   -0.282661\n 0.0279148    0.210464   -0.0179003     -0.130482   -0.723252   -0.36291\n 0.00226244   0.0990266   0.0199684      0.137001    0.0445953  -0.155039\n\n\n150×4 Matrix{Float64}:\n -4.44089e-16  -1.66533e-16  -3.29597e-16   2.23346e-16\n -4.44089e-16  -5.55112e-17   2.22045e-16  -3.33067e-16\n -4.44089e-16  -1.38778e-16  -1.07553e-16   5.58581e-16\n -4.44089e-16  -2.77556e-16  -4.3715e-16    6.66134e-16\n -4.44089e-16  -2.22045e-16  -5.55112e-16   7.77156e-16\n -4.44089e-16  -1.11022e-16  -6.38378e-16   6.45317e-16\n -4.44089e-16  -2.22045e-16  -4.44089e-16   1.54043e-15\n  0.0          -1.66533e-16  -4.4062e-16    2.22045e-16\n -4.44089e-16  -2.22045e-16  -2.18575e-16   7.80626e-16\n -4.44089e-16  -1.94289e-16  -3.33067e-16  -3.26128e-16\n -4.44089e-16   0.0          -4.44089e-16  -2.2031e-16\n  0.0          -1.8735e-16   -9.99201e-16   7.77156e-16\n -4.44089e-16  -5.55112e-17  -3.33067e-16  -3.20924e-16\n  ⋮                                        \n  2.22045e-16  -2.77556e-17   2.77556e-16   9.88792e-16\n  4.44089e-16   1.11022e-16   1.05818e-15  -3.60822e-16\n  0.0          -8.32667e-17   1.38778e-15   8.88178e-16\n  0.0           1.66533e-16   2.28983e-15   0.0\n  2.22045e-16  -1.11022e-16   2.77556e-16   1.14492e-15\n  4.44089e-16   5.55112e-17   3.33067e-16   6.52256e-16\n  0.0           5.55112e-17   1.22125e-15   1.52656e-15\n  2.22045e-16   1.66533e-16   1.88738e-15   3.33067e-16\n  2.22045e-16   2.22045e-16   1.33227e-15  -4.996e-16\n  0.0           1.249e-16     7.77156e-16   2.498e-16\n  2.22045e-16  -1.11022e-16   4.44089e-16   2.53964e-15\n  2.22045e-16  -1.11022e-16  -3.88578e-16   1.38778e-15\n\n\n4×4 Matrix{Float64}:\n  5.55112e-17   3.33067e-16   1.11022e-15  -2.66454e-15\n  1.38778e-17  -3.33067e-16  -1.55431e-15   2.16493e-15\n  3.33067e-16  -2.22045e-16  -1.91513e-15   2.77556e-16\n -3.88578e-16   2.22045e-16   3.33067e-15   2.10942e-15",
    "crumbs": [
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "PCA.html#normed-pca",
    "href": "PCA.html#normed-pca",
    "title": "Principal component analysis (PCA)",
    "section": "Normed PCA",
    "text": "Normed PCA\n\nIntroduction\n\n\\(X\\) the matrix \\((n,p)\\) of data\n\\(X_c\\) is the centered matrix\n\\(Y\\) is the centered and normed matrix. Each column of \\(Xc\\) is divided by its sample standard deviation\n\\(R=Y^TY\\) is the corretalion matrix of the Data \\(X\\).\n\\[C=U\\texttt{diag}(\\Lambda) U^T.\\] Then We have\nThe coordinates of the \\(n\\) observations in the new basis of the eigen vectors \\((\\vec{u}_1,\\ldots,\\vec{u}_p)\\) are \\[\\Psi = YU\\]\nThe The coordinates of the \\(p\\) variables in the new basis of the eigen vectors \\((\\vec{v}_1,\\ldots,\\vec{v}_p)\\) are \\[\\Phi = U\\texttt{diag}(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda}_p)\\]\nThe total inertia (variance) is \\[I = \\texttt{trace}(C)=\\sum_{i=1,p}\\lambda_i\\]\nThe variance of the variable \\(v_i\\) is \\(\\lambda_i\\)\n\n\n\nData\n\n\nCode\n# Data from Tomassone page 138 : mineral waters\nX =[341   27   3   84   23   2  \n263   23   9   91   5   3  \n287   3   5   44   24   23  \n   298   9   23   96   6   11 \n    200   15   8   70   2   4 \n    250   5   20   71   6   11 \n   357   10   2   78   24   5 \n      311   14   18   73   18   13 \n    256   6   23   86   3   18  \n   186   10   16   64   4   9 \n    183   16   44   48   11   31 \n       398   218   15   157   35   8 \n      348   51   31   140   4   14 \n   168   24   8   55   5   9 \n   110   65   5   4   1   3 \n   332   14   8   103   16   5 \n      196   18   6   58   6   13 \n       59   7   6   16   2   9 \n       402   306   15   202   36   3 \n       64   7   8   10   6   8 ]\n\ndf = DataFrame(X,[:HCO3, :SO4, :Cl, :Ca, :Mg, :Na])\ndf[:, :Origins] = [\"Aix-les-Bains\", \"Beckerish\",\n\"Cayranne\", \n\"Chambon\",\n\"Cristal-Roc\",\n\"St Cyr\",\n\"Evian\",\n\"Ferita\",\n\"St Hyppolite\",\n\"Laurier\", \n\"Ogeu\",\n\"Ondine\",\n\"Perrier\",\n\"Ribes\", \n\"Spa\",\n\"Thonon\", \n\"Veri\", \n\"Viladreau\",\n\"Vittel\", \n\"Volvic\"]\n\n\n20-element Vector{String}:\n \"Aix-les-Bains\"\n \"Beckerish\"\n \"Cayranne\"\n \"Chambon\"\n \"Cristal-Roc\"\n \"St Cyr\"\n \"Evian\"\n \"Ferita\"\n \"St Hyppolite\"\n \"Laurier\"\n \"Ogeu\"\n \"Ondine\"\n \"Perrier\"\n \"Ribes\"\n \"Spa\"\n \"Thonon\"\n \"Veri\"\n \"Viladreau\"\n \"Vittel\"\n \"Volvic\"\n\n\n\n\nGraph of the observations\n\n\nCode\nmy_PCA_results = my_PCA(X,normed=true)\nmy_Λ, my_U, my_Ψ, my_Φ, my_I_total, my_cum_var_ratio, my_R = my_PCA_results\np3 = scatter(my_Ψ[:,1],my_Ψ[:,2],xlabel = \"PC1\", ylabel=\"PC2\")\np4 = scatter(my_Ψ[:,3],my_Ψ[:,4], xlabel = \"PC1\", ylabel=\"PC2\")\nplot(p3,p4)\n\nprintln(sum(my_Φ.^2,dims=1))\nprintln(sum(my_Φ.^2,dims=2))\nprintln(\"my_Φ*my_Φ' = \")\ndisplay(my_Φ*my_Φ')\nprintln(\"Matrix of correlation = \")\ndisplay(my_R)\nprintln(\"my_Φ'*my_Φ = \")\ndisplay(my_Φ'*my_Φ)\n\n\n[3.0940874688062925 1.6875603215911315 0.5965131904043633 0.5028441637469864 0.09323922489032904 0.025755630560899054]\n[0.9999999999999994; 0.9999999999999997; 1.0000000000000007; 0.9999999999999999; 1.0000000000000007; 1.0000000000000018;;]\nmy_Φ*my_Φ' = \n\n\n6×6 Matrix{Float64}:\n  1.0        0.47796     0.121776    0.851749   0.730575   -0.108825\n  0.47796    1.0         0.0449271   0.732646   0.670631   -0.278783\n  0.121776   0.0449271   1.0         0.252035  -0.125463    0.668014\n  0.851749   0.732646    0.252035    1.0        0.605682   -0.196195\n  0.730575   0.670631   -0.125463    0.605682   1.0        -0.0905507\n -0.108825  -0.278783    0.668014   -0.196195  -0.0905507   1.0\n\n\nMatrix of correlation = \n\n\n6×6 Matrix{Float64}:\n  1.0        0.47796     0.121776    0.851749   0.730575   -0.108825\n  0.47796    1.0         0.0449271   0.732646   0.670631   -0.278783\n  0.121776   0.0449271   1.0         0.252035  -0.125463    0.668014\n  0.851749   0.732646    0.252035    1.0        0.605682   -0.196195\n  0.730575   0.670631   -0.125463    0.605682   1.0        -0.0905507\n -0.108825  -0.278783    0.668014   -0.196195  -0.0905507   1.0\n\n\nmy_Φ'*my_Φ = \n\n\n6×6 Matrix{Float64}:\n  3.09409       3.94656e-16   1.3417e-16   …   9.19712e-17   5.2522e-17\n  3.94656e-16   1.68756       5.89447e-16      8.47945e-17  -7.23998e-18\n  1.3417e-16    5.89447e-16   0.596513        -2.69255e-17  -6.3729e-18\n -1.54354e-16   3.39583e-16   3.4386e-16      -1.29052e-17   8.19281e-18\n  9.19712e-17   8.47945e-17  -2.69255e-17      0.0932392     4.41758e-18\n  5.2522e-17   -7.23998e-18  -6.3729e-18   …   4.41758e-18   0.0257556\n\n\n\n\nGraph of the variables\n\n\nCode\nn,p = size(X)\nprintln(\"p = \", p)\nNames = names(df)\nprintln(Names)\npvar1 = plot()\nech = 1.1*maximum(abs.(my_Φ))\nfor i=1:p\n    plot!(pvar1,[0,my_Φ[i,1]], [0,my_Φ[i,2]], xlims=(-ech,ech), ylims=(-ech,ech), arrow=true, label=Names[i], legend=:bottomleft, xlabel=\"v1\", ylabel=\"v2\")\nend\n\n# Plot the unit cercle\ncercle(θ)=[cos.(θ) sin.(θ)]\nInter_theta = 0:0.01:2*π\nCercle = cercle(Inter_theta)\nplot!(pvar1,Cercle[:,1],Cercle[:,2])\n\npvar2 = plot()\nfor i=1:p\n    plot!(pvar2,[0,my_Φ[i,3]], [0,my_Φ[i,4]], xlims=(-ech,ech), ylims=(-ech,ech), arrow=true, label=Names[i], legend=:bottomleft, xlabel=\"v3\", ylabel=\"v4\")\nend\nplot!(pvar2,Cercle[:,1],Cercle[:,2])\nplot(pvar1,pvar2,aspect_ratio=:equal)\n\n\np = 6\n[\"HCO3\", \"SO4\", \"Cl\", \"Ca\", \"Mg\", \"Na\", \"Origins\"]",
    "crumbs": [
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "PCA.html#colored-image-to-the-best-black-and-white",
    "href": "PCA.html#colored-image-to-the-best-black-and-white",
    "title": "Principal component analysis (PCA)",
    "section": "Colored Image to the best black and white",
    "text": "Colored Image to the best black and white\n\nData\n\n\nCode\nusing Plots\nusing TestImages, Images\nimg = testimage(\"lighthouse\")\nimgarr = channelview(img)\nplot(img)\nprintln(imgarr[1,1:3,1:3])\nimg_red = StackedView(imgarr[1,:,:], zeroarray, zeroarray)\nimg2_red = colorview(RGB, img_red)\nimg_green = StackedView(zeroarray,imgarr[2,:,:], zeroarray)\nimg2_green = colorview(RGB, img_green)\nimg_blue = StackedView(zeroarray, zeroarray,imgarr[3,:,:])\nimg2_blue = colorview(RGB, img_blue)\n#permutedims(imgarr, [2,3,1])\nmosaicview(img,img2_red, img2_green, img2_blue; nrow = 2)\n\n\nN0f8[0.361N0f8 0.349N0f8 0.345N0f8; 0.361N0f8 0.361N0f8 0.361N0f8; 0.376N0f8 0.353N0f8 0.361N0f8]\n\n\n\n\n\n\n\nPCA\n\n\nCode\nX1=imgarr[1,:,:];X2=imgarr[2,:,:];X3=imgarr[3,:,:];\nnl,nc=size(X1);\nprintln(nl,\" \", nc)\n#\n# X is the matrix for the PCA\nX=[X1[:] X2[:] X3[:]];\nprintln(size(X))\nΛ, U, Ψ, Φ, I_total, cum_var_ratio, cov_mat = my_PCA(X)\nprintln(\"nl = \", nl)\nprintln(\"nc = \", nc)\nprintln(size(Ψ))\nΨ1 = zeros(nl,nc)\nΨ2 = zeros(nl,nc)\nΨ3 = zeros(nl,nc)\nprintln(size(Ψ1))\nI=1:nl;\nfor j in 1:nc\n   Ψ1[I,j] = Ψ[(j-1)*nl .+ I,1];\n   Ψ2[I,j] = Ψ[(j-1)*nl .+ I,2];\n   Ψ3[I,j] = Ψ[(j-1)*nl .+ I,3];\nend\n\n\nminΨ1=minimum(Ψ1)\nmaxΨ1=maximum(Ψ1)\nPSI1=(Ψ1 .- minΨ1) ./ (maxΨ1-minΨ1)\n\nusing Colors\nGray.(PSI1)\n#@view PSI1\n\n\n512 768\n(393216, 3)\nnl = 512\nnc = 768\n(393216, 3)\n(512, 768)",
    "crumbs": [
      "Principal component analysis (PCA)"
    ]
  },
  {
    "objectID": "optimization.html",
    "href": "optimization.html",
    "title": "Optimization",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbf{R}}}\n\\newcommand{\\abs}[1]{\\lvert#1\\rvert}\n\\newcommand{\\norme}[1]{\\lVert#1\\rVert}\n\\]",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "optimization.html#introduction",
    "href": "optimization.html#introduction",
    "title": "Optimization",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\nPierre de Fermat\n\n\n\n\n\nPierre de Fermat, 1601 (Beaumont-de-Lomagne, near Montauban) – 1665 (Castres)\nmethod maximis et minimis\nEarly developments that led to infinitesimal calculus\n\n\n\n\nThe problem is to solve\n\\[(P)\\left\\{\\begin{array}{l}\nMin\\; f(x)\\\\\nx\\in\\R^n\n\\end{array}\\right.\n\\]",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "optimization.html#automatique-differentiation",
    "href": "optimization.html#automatique-differentiation",
    "title": "Optimization",
    "section": "Automatique differentiation",
    "text": "Automatique differentiation\n\n\nCode\nusing LinearAlgebra\nusing ForwardDiff\nA = [-1 2;1 4]; b=[1,1]; c=1\nf(x) = 0.5*x'*A*x + b'*x +c\n\nanalytic_∇f(x) = 0.5*(A' + A)*x+b\nanalytic_∇²f(x) = 0.5*(A' + A)\n\n∇f(x) = ForwardDiff.gradient(f, x)\n∇²f(x) = ForwardDiff.hessian(f, x)\n\nx0 = [1,-1]\nprintln(\"∇f(x0) = \", ∇f(x0))\nprintln(\"analytic_∇f(x0) = \", analytic_∇f(x0))\nprintln(\"analytic_∇f(x0) - ∇f(x0) = \", analytic_∇f(x0)- ∇f(x0))\n\nprintln(\"∇²f(x0) = \", ∇²f(x0))\nprintln(\"analytic_∇²f(x0) = \", analytic_∇²f(x0))\nprintln(\"analytic_∇²f(x0) - ∇²f(x0) = \", analytic_∇²f(x0)- ∇²f(x0))\n\n\n┌ Error: Error during loading of extension SpecialFunctionsExt of ColorVectorSpace, use `Base.retry_load_extensions()` to retry.\n│   exception =\n│    1-element ExceptionStack:\n│    ArgumentError: Package SpecialFunctionsExt [997ecda8-951a-5f50-90ea-61382e97704b] is required but does not seem to be installed:\n│     - Run `Pkg.instantiate()` to install all recorded dependencies.\n│    \n│    Stacktrace:\n│      [1] _require(pkg::Base.PkgId, env::Nothing)\n│        @ Base ./loading.jl:2438\n│      [2] __require_prelocked(uuidkey::Base.PkgId, env::Nothing)\n│        @ Base ./loading.jl:2315\n│      [3] #invoke_in_world#3\n│        @ ./essentials.jl:1089 [inlined]\n│      [4] invoke_in_world\n│        @ ./essentials.jl:1086 [inlined]\n│      [5] _require_prelocked\n│        @ ./loading.jl:2302 [inlined]\n│      [6] _require_prelocked\n│        @ ./loading.jl:2301 [inlined]\n│      [7] run_extension_callbacks(extid::Base.ExtensionId)\n│        @ Base ./loading.jl:1500\n│      [8] run_extension_callbacks(pkgid::Base.PkgId)\n│        @ Base ./loading.jl:1535\n│      [9] run_package_callbacks(modkey::Base.PkgId)\n│        @ Base ./loading.jl:1354\n│     [10] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128, stalecheck::Bool; reasons::Dict{String, Int64}, DEPOT_PATH::Vector{String})\n│        @ Base ./loading.jl:1993\n│     [11] _require(pkg::Base.PkgId, env::String)\n│        @ Base ./loading.jl:2450\n│     [12] __require_prelocked(uuidkey::Base.PkgId, env::String)\n│        @ Base ./loading.jl:2315\n│     [13] #invoke_in_world#3\n│        @ ./essentials.jl:1089 [inlined]\n│     [14] invoke_in_world\n│        @ ./essentials.jl:1086 [inlined]\n│     [15] _require_prelocked(uuidkey::Base.PkgId, env::String)\n│        @ Base ./loading.jl:2302\n│     [16] macro expansion\n│        @ ./loading.jl:2241 [inlined]\n│     [17] macro expansion\n│        @ ./lock.jl:273 [inlined]\n│     [18] __require(into::Module, mod::Symbol)\n│        @ Base ./loading.jl:2198\n│     [19] #invoke_in_world#3\n│        @ ./essentials.jl:1089 [inlined]\n│     [20] invoke_in_world\n│        @ ./essentials.jl:1086 [inlined]\n│     [21] require(into::Module, mod::Symbol)\n│        @ Base ./loading.jl:2191\n│     [22] eval\n│        @ ./boot.jl:430 [inlined]\n│     [23] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n│        @ Base ./loading.jl:2643\n│     [24] softscope_include_string(m::Module, code::String, filename::String)\n│        @ SoftGlobalScope ~/.julia/packages/SoftGlobalScope/u4UzH/src/SoftGlobalScope.jl:65\n│     [25] execute_request(socket::ZMQ.Socket, msg::IJulia.Msg)\n│        @ IJulia ~/.julia/packages/IJulia/dR0lE/src/execute_request.jl:74\n│     [26] #invokelatest#2\n│        @ ./essentials.jl:1055 [inlined]\n│     [27] invokelatest\n│        @ ./essentials.jl:1052 [inlined]\n│     [28] eventloop(socket::ZMQ.Socket)\n│        @ IJulia ~/.julia/packages/IJulia/dR0lE/src/eventloop.jl:8\n│     [29] (::IJulia.var\"#15#18\")()\n│        @ IJulia ~/.julia/packages/IJulia/dR0lE/src/eventloop.jl:38\n└ @ Base loading.jl:1506\n\n\n∇f(x0) = [-1.5, -1.5]\nanalytic_∇f(x0) = [-1.5, -1.5]\nanalytic_∇f(x0) - ∇f(x0) = [0.0, 0.0]\n∇²f(x0) = [-1.0 1.5; 1.5 4.0]\nanalytic_∇²f(x0) = [-1.0 1.5; 1.5 4.0]\nanalytic_∇²f(x0) - ∇²f(x0) = [0.0 0.0; 0.0 0.0]",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "optimization.html#newtons-algorithm",
    "href": "optimization.html#newtons-algorithm",
    "title": "Optimization",
    "section": "Newton’s Algorithm",
    "text": "Newton’s Algorithm\nSolve \\(\\nabla f(x) = 0\\) by Newton’s method\nRequire \\(f \\colon \\R^n \\to \\R\\), \\(x_{0} \\in \\R^n\\) ( initial point)\n\n\\(k \\gets 0\\)\ncontinue = true\nWhile continue \\(\\#\\) See Section Section 1.3.1\n\n$d_k $ solution of \\(\\nabla ^2f(x_k)\\, d =-\\nabla f(x\\_k)\\) \\(\\#\\) Newton’s direction\n\\(x_{k+1} \\gets x_{k} + d_{k}\\) # Mise à jour de l’itéré\n\\(k \\gets k + 1\\)\ncontinue = stop_function(\\(\\nabla f_k\\),\\(x_k\\),\\(x_{k+1}\\),\\(f_k\\),\\(f_{k+1}\\),AbsTol,RelTol,\\(\\varepsilon\\))\n\nEndWhile\n\n\nStop criteria\n\nStop criteria, Stagnation criteria are more restrictive (\\(\\varepsilon = 0.01\\), for example).\n\n\n\n\n\n\nCriteria\nFormula\n\n\n\n\n\\(\\norme{\\nabla f(x_{k+1})}=0\\)\n\\(\\norme{\\nabla f(x_{k+1})} &lt; \\mathrm{max}(\\mathrm{RelTol}\\norme{\\nabla f(x_0)},\\mathrm{AbsTol})\\)\n\n\nStagnation of the iterate\n\\(\\norme{x_{k+1}-x_k} &lt; \\varepsilon\\mathrm{max}(\\mathrm{RelTol}\\norme{x_{k+1}},\\mathrm{AbsTol})\\)\n\n\nStagnation of the function\n\\(\\abs{f(x_{k+1})-f(x_k)} &lt; \\varepsilon\\mathrm{max}(\\mathrm{RelTol}\\abs{f(x_{k+1})},\\mathrm{AbsTol})\\)\n\n\nMaximum number of iteration\n\\(k+1 = \\mathrm{max\\_iter}\\)\n\n\n\n\n\nExercice\n\n\nCode\nusing LinearAlgebra\nusing ForwardDiff\n\n\"\"\"\n   Solve by Newton's algorithm the optimization problem Min f(x)\n   Case where f is a function from R^n to R\n\"\"\"\n\n\nfunction algo_Newton(f,x0::Vector{&lt;:Real};AbsTol= abs(eps()), RelTol = abs(eps()), ε=0.01, nbit_max = 0)\n# to complete\n    \n    # flag = 0 if the program stop on the first criteria\n    # flag = 2 if the program stop on the second criteria\n    # ...\n    return xₖ, flag, fₖ, ∇fₖ , k  \nend\n\n\nalgo_Newton (generic function with 1 method)\n\n\n\n\nCode\ninclude(\"assets/julia/MyOptims.jl\")\n\nA = [1 0 ; 0 9/2]\nb = [0,0]; c=0.\nf1(x) = 0.5*x'*A*x + b'*x +c\nx0 = [1000,-20]\nprintln(\"Results for Newton on f1 : \", algo_Newton(f1,x0))\nprintln(\"eigen value of 0.5(A^T+A) = \", 0.5*eigen(A'+A).values)\nusing Plots;\nx=range(-10,stop=10,length=100)\ny=range(-10,stop=10,length=100)\nf11(x,y) = f1([x,y])\np1 = plot(x,y,f11,st=:contourf)\n\nA = [-1 0 ; 0 3]\nf2(x) = 0.5*x'*A*x + b'*x +c\nprintln(\"Results for Newton on f2 : \", algo_Newton(f2,x0))\nprintln(\"eigen value of 0.5(A^T+A) = \", 0.5*eigen(A'+A).values)\nf21(x,y) = f1([x,y])\np2 = plot(x,y,f21,st=:contourf)\n# Rosenbrock function\nx=range(-1.5,stop=2,length=100)\ny=range(-0.5,stop=3.,length=100)\nf3(x) = (1-x[1])^2 + 100*(x[2]-x[1]^2)^2\nf31(x,y) = f3([x,y])\np3 = plot(x,y,f31,st=:contourf)\nx0 = [1.1,1.2]\n\nprintln(\"Results for Newton on f3 : \", algo_Newton(f3,[1.1,1.2]))\n\nprintln(\"Results for Newton on f3 : \", algo_Newton(f3,[3.,0.]))\nplot(p1,p2,p3)\n\n\nResults for Newton on f1 : ([0.0, 0.0], 0, 0.0, [0.0, 0.0], 1)\neigen value of 0.5(A^T+A) = [1.0, 4.5]\nResults for Newton on f2 : ([0.0, 0.0], 0, 0.0, [0.0, 0.0], 1)\neigen value of 0.5(A^T+A) = [-1.0, 3.0]\nResults for Newton on f3 : ([1.0, 1.0], 0, 0.0, [-0.0, 0.0], 7)\nResults for Newton on f3 : ([1.0, 1.0], 0, 0.0, [-0.0, 0.0], 5)",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "optimization.html#steepest-descent",
    "href": "optimization.html#steepest-descent",
    "title": "Optimization",
    "section": "Steepest descent",
    "text": "Steepest descent\n\nAlgorithm\nRequire \\(f \\colon \\R^n \\to \\R\\), \\(x_{0} \\in \\R^n\\) ( initial point)\n\n\\(k \\gets 0\\)\ncontinue = true\nWhile continue \\(\\#\\) See Section Section 1.3.1\n\n\\(d_k = -\\nabla f(x_k)\\)\n\\(\\alpha_k = argmin_{\\alpha&gt;0}\\{f(x_k+\\alpha d_k)\\}\\)\n\\(x_{k+1} \\gets x_{k} + \\alpha_kd_{k}\\)\n\n\\(k \\gets k + 1\\)\ncontinue = stop_function(\\(\\nabla f_k\\),\\(x_k\\),\\(x_{k+1}\\),\\(f_k\\),\\(f_{k+1}\\),AbsTol,RelTol,\\(\\varepsilon\\))\n\nEndWhile\n\n\n\nSteepest descent for a quadratic function\n\n\n\n\n\n\nExercise:\n\n\n\nLet \\(f(x)=x^TAx +b^Tx + c\\), where \\(A\\) symetric and positive-definite.\nIf \\(x_k\\) is a fixed vector and \\(d_k\\) a nonzero vector compute \\(\\alpha^*\\) the solution of \\[Min\\; g(\\alpha)=f(x_k+\\alpha d_k)\\]\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\\[\\alpha^* = -\\frac{2x_k^TAd_k + b^Td_k}{2d_k^TAd_k}\\]\n\n\n\n\n\n\n\n\n\nExercise:\n\n\n\nComplete the steepest_descent_quad function and execute the following sript and explain why the Newton’s algorithm converge in one iteration.\n\n\n\n\nA = [1.0 0.0; 0.0 9.0]\nb = [-4.0, -18.0]\nn= 5\n\nResult with Newton's algorithm : \nxsol = [2.0, 1.0]\nflag = 0\nfsol = 0.0\n∇f_xsol = [0.0, 0.0]\nnb_iter = 1\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear search: Armijo’s condition and backtraking\nLet \\(x_k\\) the current iterate and \\(d_k=-\\nabla f(x_k)\\). We note \\(g_k(\\alpha)=f(x_k+\\alpha d_k)\\) We want to find an \\(\\alpha&gt;0\\) such that we have a sufficient decay of the function \\(g_k\\), i.e. which verify the Armijo’s condition : \\(g_f(\\alpha)=f(x_k+\\alpha d_k)\\le f_k+c_1\\alpha\\nabla f_k^Td_k=\\tilde{g}_f(\\alpha)\\)\n\nInitialization\n\n\\(\\alpha_0=1\\)\n\\(\\rho\\in]0,1[\\) (\\(\\rho=0.8\\))\n\\(c\\in]0,1[\\) (\\(c=10^{-4}\\))\n\\(k=0\\)\n\nWhile \\(g_{f}(\\alpha_k)&gt;\\tilde{g}_f(\\alpha_k)=f_k+c_1\\alpha_k\\nabla f_k^Td_k\\)\n\n\\(\\alpha_k:=\\rho\\alpha_k\\)\n\\(k:=k+1\\)\n\nend\n\n\n\n\n\n\n\nExercise: Complete the descent_backtrac function and execute the following sript.\n\n\n\n\n\nCode\nusing Plots\nx = range(-10*(n/2)-xsol[1],stop=10*(n/2)-xsol[1],length=100)\ny = range(-9*(n/2)-xsol[2],stop=9*(n/2)-xsol[2],length=100)\nf_contour(x,y) = f([x,y])\nz = @. f_contour(x', y)\nnb_levels = 7\nx0 = (n/2)*[9,1]\nxₖ = x0\np1 = plot()\nfor nbit in 1:nb_levels\n  xsol, flag, fsol, ∇f_xsol , nb_iter  = descent_backtrac(f,x0,nbit_max=nbit)\n  #xsol, flag, fsol, ∇f_xsol , nb_iter  = my_descent(f,x0,nbit_max=nbit)\n  println(\"xsol = \", xsol)\n  println(\"flag = \", flag)\n  println(\"fsol = \", fsol)\n  println(\"∇f_xsol = \", ∇f_xsol)\n  println(\"nb_iter = \", nb_iter)\n  plot!(p1,[xₖ[1],xsol[1]],[xₖ[2],xsol[2]],arrow=true)\n  xₖ = xsol\nend\nlevels = [f(Xsol[k,:]) for k in nb_levels+1:-1:1]\ncontour!(p1,xx,yy,z,levels=levels,cbar=false,color=:turbo)\nplot(p1,legend=false)\n\n\nxsol = [11.752095999999996, -4.577888000000002]\nflag = 3\nfsol = 375.1188872581122\n∇f_xsol = [19.504191999999993, -100.40198400000004]\nnb_iter = 1\nxsol = [9.657849330627375, 6.2026929433378895]\nflag = 3\nfsol = 302.25478113451095\n∇f_xsol = [15.31569866125475, 93.648472980082]\nnb_iter = 2\nxsol = [8.013338708990371, -3.8527352759069133]\nflag = 3\nfsol = 248.1015993513241\n∇f_xsol = [12.026677417980743, -87.34923496632445]\nnb_iter = 3\nxsol = [6.721984054246145, 5.52631741186767]\nflag = 3\nfsol = 206.68507722534292\n∇f_xsol = [9.44396810849229, 81.47371341361807]\nnb_iter = 4\nxsol = [5.70794569998511, -3.221855953011487]\nflag = 3\nfsol = 174.16547050584487\n∇f_xsol = [7.415891399970221, -75.99340715420678]\nnb_iter = 5\nxsol = [4.911670424146116, 4.93787400796178]\nflag = 3\nfsol = 148.03948998207602\n∇f_xsol = [5.823340848292233, 70.88173214331205]\nnb_iter = 6\nxsol = [4.286393961724215, -2.6729940280221545]\nflag = 3\nfsol = 126.64556351718664\n∇f_xsol = [4.5727879234484305, -66.11389250439878]\nnb_iter = 7",
    "crumbs": [
      "Optimization"
    ]
  },
  {
    "objectID": "NN.html",
    "href": "NN.html",
    "title": "Neurol Network”",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbf{R}}}\n\\newcommand{\\abs}[1]{\\lvert#1\\rvert}\n\\newcommand{\\norme}[1]{\\lVert#1\\rVert}\n\\]",
    "crumbs": [
      "Neurol Network\""
    ]
  },
  {
    "objectID": "NN.html#descent-algorithm-with-constant-rate",
    "href": "NN.html#descent-algorithm-with-constant-rate",
    "title": "Neurol Network”",
    "section": "Descent algorithm with constant rate",
    "text": "Descent algorithm with constant rate\n\nAlgorithm\nRequire \\(f \\colon \\R^n \\to \\R\\), \\(x_{0} \\in \\R^n\\) ( initial point)\n\n\\(\\eta = 0.1\\)\n\\(k \\gets 0\\)\ncontinue = true\nWhile continue\n\n\\(d_k = -\\nabla f(x_k)\\)\n\\(x_{k+1} \\gets x_{k} + \\eta d_{k}\\)\n\n\\(k \\gets k + 1\\)\ncontinue = stop_function(\\(\\nabla f_k\\),\\(x_k\\),\\(x_{k+1}\\),\\(f_k\\),\\(f_{k+1}\\),AbsTol,RelTol,\\(\\varepsilon\\))\n\nEndWhile\n\n\n\nApplication : Simple Linear Regression\n\n\nCode\nx1 = 1.; x2 = sqrt(5*9/2-x1^2);\nx = [-x2, -x1, 0., x1, x2]\n\nn = length(x)\nX = [ones(n)  x]\n\na₁ = 1; a₀ = 2\ny = a₁*x .+ a₀ # model\n\ninclude(\"assets/julia/MyOptims.jl\")\nA = X'*X\nprintln(\"A = \", A)\nb = -X'*y\nprintln(\"b = \", b)\nprintln(\"n= \",n)\nx0 = (n/2)*[9,1]\nprintln(\"X*x0 = \", X*x0)\nf(x) = (2/n)*(0.5*x'*A*x + b'*x + 0.5*y'*y) \n\nprintln(\"x0 = \", x0)\nprintln(\"f(x0) = \", f(x0))\nxₖ = x0\n#p1 = plot()\nnb_levels = 7\nXsol = zeros(nb_levels+1,2)\nXsol[1,:] = x0\nfor nbit in 1:nb_levels\n  xsol, flag, fsol, ∇f_xsol , nb_iter  = my_descent(f,x0,nbit_max=nbit)\n  println(\"xsol = \", xsol)\n  xₖ = xsol\n  Xsol[nbit+1,:] = xsol\nend\n\n\n┌ Error: Error during loading of extension SpecialFunctionsExt of ColorVectorSpace, use `Base.retry_load_extensions()` to retry.\n│   exception =\n│    1-element ExceptionStack:\n│    ArgumentError: Package SpecialFunctionsExt [997ecda8-951a-5f50-90ea-61382e97704b] is required but does not seem to be installed:\n│     - Run `Pkg.instantiate()` to install all recorded dependencies.\n│    \n│    Stacktrace:\n│      [1] _require(pkg::Base.PkgId, env::Nothing)\n│        @ Base ./loading.jl:2438\n│      [2] __require_prelocked(uuidkey::Base.PkgId, env::Nothing)\n│        @ Base ./loading.jl:2315\n│      [3] #invoke_in_world#3\n│        @ ./essentials.jl:1089 [inlined]\n│      [4] invoke_in_world\n│        @ ./essentials.jl:1086 [inlined]\n│      [5] _require_prelocked\n│        @ ./loading.jl:2302 [inlined]\n│      [6] _require_prelocked\n│        @ ./loading.jl:2301 [inlined]\n│      [7] run_extension_callbacks(extid::Base.ExtensionId)\n│        @ Base ./loading.jl:1500\n│      [8] run_extension_callbacks(pkgid::Base.PkgId)\n│        @ Base ./loading.jl:1535\n│      [9] run_package_callbacks(modkey::Base.PkgId)\n│        @ Base ./loading.jl:1354\n│     [10] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128, stalecheck::Bool; reasons::Dict{String, Int64}, DEPOT_PATH::Vector{String})\n│        @ Base ./loading.jl:1993\n│     [11] _require(pkg::Base.PkgId, env::String)\n│        @ Base ./loading.jl:2450\n│     [12] __require_prelocked(uuidkey::Base.PkgId, env::String)\n│        @ Base ./loading.jl:2315\n│     [13] #invoke_in_world#3\n│        @ ./essentials.jl:1089 [inlined]\n│     [14] invoke_in_world\n│        @ ./essentials.jl:1086 [inlined]\n│     [15] _require_prelocked(uuidkey::Base.PkgId, env::String)\n│        @ Base ./loading.jl:2302\n│     [16] macro expansion\n│        @ ./loading.jl:2241 [inlined]\n│     [17] macro expansion\n│        @ ./lock.jl:273 [inlined]\n│     [18] __require(into::Module, mod::Symbol)\n│        @ Base ./loading.jl:2198\n│     [19] #invoke_in_world#3\n│        @ ./essentials.jl:1089 [inlined]\n│     [20] invoke_in_world\n│        @ ./essentials.jl:1086 [inlined]\n│     [21] require(into::Module, mod::Symbol)\n│        @ Base ./loading.jl:2191\n│     [22] include(fname::String)\n│        @ Main ./sysimg.jl:38\n│     [23] top-level scope\n│        @ In[3]:10\n│     [24] eval\n│        @ ./boot.jl:430 [inlined]\n│     [25] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n│        @ Base ./loading.jl:2643\n│     [26] softscope_include_string(m::Module, code::String, filename::String)\n│        @ SoftGlobalScope ~/.julia/packages/SoftGlobalScope/u4UzH/src/SoftGlobalScope.jl:65\n│     [27] execute_request(socket::ZMQ.Socket, msg::IJulia.Msg)\n│        @ IJulia ~/.julia/packages/IJulia/dR0lE/src/execute_request.jl:74\n│     [28] #invokelatest#2\n│        @ ./essentials.jl:1055 [inlined]\n│     [29] invokelatest\n│        @ ./essentials.jl:1052 [inlined]\n│     [30] eventloop(socket::ZMQ.Socket)\n│        @ IJulia ~/.julia/packages/IJulia/dR0lE/src/eventloop.jl:8\n│     [31] (::IJulia.var\"#15#18\")()\n│        @ IJulia ~/.julia/packages/IJulia/dR0lE/src/eventloop.jl:38\n└ @ Base loading.jl:1506\n\n\nA = [5.0 0.0; 0.0 45.0]\nb = [-10.0, -45.0]\nn= 5\nX*x0 = [10.90797688063037, 20.0, 22.5, 25.0, 34.09202311936963]\nx0 = [22.5, 2.5]\nf(x0) = 440.5\nxsol = [18.4, -0.20000000000000018]\nxsol = [15.119999999999997, 1.9600000000000004]\nxsol = [12.495999999999997, 0.23199999999999954]\nxsol = [10.396799999999997, 1.6144000000000003]\nxsol = [8.717439999999998, 0.5084799999999998]\nxsol = [7.373951999999998, 1.3932160000000002]\nxsol = [6.299161599999998, 0.6854271999999997]",
    "crumbs": [
      "Neurol Network\""
    ]
  },
  {
    "objectID": "NN.html#with-the-flux-package",
    "href": "NN.html#with-the-flux-package",
    "title": "Neurol Network”",
    "section": "With the Flux Package",
    "text": "With the Flux Package\nFind the same results with Flux (see https://fluxml.ai/Flux.jl/stable/)\n\n\nCode\nx_train = reshape(Float32.(x),1,n)\nprintln(\"x_train = \", x_train)\ny_train = reshape(Float32.(y),1,n)\nprintln(\"y_train = \", y_train)\nusing Flux, Statistics\nmodel = Dense(1 =&gt; 1)\nmodel.weight[1,1]=Float32((n/2))\nmodel.bias[1] = Float32((n/2)*9)\nprintln(\"model.weight = \", model.weight)\nprintln(\"model.bias = \", model.bias)\nprintln(\"model(x_train) = \", model(x_train))\nloss(model, x, y) = mean(abs2.(model(x) .- y));\nprintln(\"loss(model, x_train, y_train) = \", loss(model, x_train, y_train))\nprintln(abs2.(model(x_train) .- y_train))\n\nopt = Descent()\ndata = [(x_train, y_train)]\nprintln(\"data = \", data)\nprintln(\"n= \",n)\n#data = Flux.DataLoader((x_train, y_train), batchsize=5)\nprintln(\"data = \", first(data))\n\nXsolNN = zeros(nb_levels+1,2)\nXsolNN[1,:] = x0\nprintln(\"Flux.setup(rule, model) = \", Flux.setup(opt, model))\ndLdm, _, _ = gradient(loss, model, x_train, y_train)\nprintln(\"dLdm = \", dLdm)\nfor epoch in 1:nb_levels\n    Flux.train!(loss, model, data, opt)\n    println(\"model.weight = \", model.weight)\n    println(\"model.bias = \", model.bias)\n    XsolNN[epoch+1,:] = [model.bias[1], model.weight[1,1]]\nend\n\nprintln(\"Xsol = \", Xsol)\nprintln(\"XsolNN = \", XsolNN)\nprintln(\"Xsol - XsolNN\")\ndisplay(Xsol - XsolNN)\n\n\nx_train = Float32[-4.6368093 -1.0 0.0 1.0 4.6368093]\ny_train = Float32[-2.6368093 1.0 2.0 3.0 6.6368093]\nmodel.weight = Float32[2.5;;]\nmodel.bias = Float32[22.5]\nmodel(x_train) = Float32[10.907976 20.0 22.5 25.0 34.092026]\nloss(model, x_train, y_train) = 440.5\nFloat32[183.46121 361.0 420.25 484.0 753.7889]\ndata = Tuple{Matrix{Float32}, Matrix{Float32}}[([-4.6368093 -1.0 0.0 1.0 4.6368093], [-2.6368093 1.0 2.0 3.0 6.6368093])]\nn= 5\ndata = (Float32[-4.6368093 -1.0 0.0 1.0 4.6368093], Float32[-2.6368093 1.0 2.0 3.0 6.6368093])\nFlux.setup(rule, model) = (weight = Leaf(Descent(0.1), nothing), bias = Leaf(Descent(0.1), nothing), σ = ())\ndLdm = (weight = Float32[27.000004;;], bias = Float32[41.0], σ = nothing)\nmodel.weight = Float32[-0.20000052;;]\nmodel.bias = Float32[18.4]\nmodel.weight = Float32[1.9600008;;]\nmodel.bias = Float32[15.12]\nmodel.weight = Float32[0.23199975;;]\nmodel.bias = Float32[12.496]\nmodel.weight = Float32[1.6144003;;]\nmodel.bias = Float32[10.3968]\nmodel.weight = Float32[0.50847995;;]\nmodel.bias = Float32[8.717441]\nmodel.weight = Float32[1.3932161;;]\nmodel.bias = Float32[7.3739524]\nmodel.weight = Float32[0.685427;;]\nmodel.bias = Float32[6.299162]\nXsol = [22.5 2.5; 18.4 -0.20000000000000018; 15.119999999999997 1.9600000000000004; 12.495999999999997 0.23199999999999954; 10.396799999999997 1.6144000000000003; 8.717439999999998 0.5084799999999998; 7.373951999999998 1.3932160000000002; 6.299161599999998 0.6854271999999997]\nXsolNN = [22.5 2.5; 18.399999618530273 -0.20000052452087402; 15.119999885559082 1.96000075340271; 12.496000289916992 0.23199975490570068; 10.39680004119873 1.6144002676010132; 8.717440605163574 0.5084799528121948; 7.373952388763428 1.3932161331176758; 6.299161911010742 0.6854270100593567]\nXsol - XsolNN\n\n\n8×2 Matrix{Float64}:\n  0.0          0.0\n  3.8147e-7    5.24521e-7\n  1.14441e-7  -7.53403e-7\n -2.89917e-7   2.45094e-7\n -4.11987e-8  -2.67601e-7\n -6.05164e-7   4.71878e-8\n -3.88763e-7  -1.33118e-7\n -3.11011e-7   1.89941e-7",
    "crumbs": [
      "Neurol Network\""
    ]
  },
  {
    "objectID": "NN.html#mnist",
    "href": "NN.html#mnist",
    "title": "Neurol Network”",
    "section": "MNIST",
    "text": "MNIST\nThis example comes from the book Statistics with julia, Yoni Nazarathy and Hayden Klol, Springer, 2021",
    "crumbs": [
      "Neurol Network\""
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julia for Statistics",
    "section": "",
    "text": "Julia For Statistics",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "least-squares-pb.html",
    "href": "least-squares-pb.html",
    "title": "Least Squares Probem",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbf{R}}}\n\\newcommand{\\abs}[1]{\\lvert#1\\rvert}\n\\newcommand{\\norme}[1]{\\lVert#1\\rVert}\n\\]\n\n\nLeast Squares Probem\n\n\n\n\n\n\nExample : $C^{14} Datation\n\n\n\nRadioactive carbon \\(^{14}C\\) is produced in the atmosphere by the effect of cosmic rays on atmospheric nitrogen. It is oxidized to \\(^{14}CO_{2}\\) and absorbed in this form by living organisms. So, living organisms contain a certain percentage of radioactive carbon relative to \\(^{12}C\\) and \\(^{13}C\\) which are stable. We suppose that carbon production \\(^{14}C\\) is constant over the last few millennia.\nIt is also assumed that, when an organism dies, its exchanges with the atmosphere cease, and that radioactivity due to carbon to carbon \\(^{14}C\\) decreases according to the following exponential law:\n\\[\nA(t,A_0,\\lambda)=A_{0}e^{-\\lambda t}.\n\\]\nThe analysis of the trunks (wood is dead tissue) of old trees {} and {} furnishes us~:\n\nits age \\(t\\) in year\nits radioactivity \\(A\\)\n\n\\[\n\\begin{array}{||c|ccccccc||}\\hline\\hline\nt_i & 500 & 1000 & 2000 & 3000 & 4000 & 5000 & 6300 \\\\ \\hline\nA_i & 14.5 & 13.5 & 12.0 & 10.8 & 9.9 & 8.9 & 8.0\\\\ \\hline\\hline\n\\end{array}\n\\]\nWe want to find the values of the parameters \\(A_0\\) and \\(\\lambda\\), so that the function \\(A{(t,A_0,\\lambda)}\\) is “near” the data :\n\\[(P) \\left\\{   \n  \\begin{array}{l}\n    \\displaystyle Min\\; f(\\beta) = \\frac{1}{2}\\|r(\\beta\\|^2 = f(A_0,\\lambda)=\\frac{1}{2}\\sum_{i=1}^{n}(A_i-A_0e^{-\\lambda t_i})^2\\\\\n    \\beta=(A_0,\\lambda) \\in  {\\R}^2.\n  \\end{array} \\right.\n  \\]\nSolve this problem by using :\n\nThe Newton algorithm\nThe Gauss-Newton algorithm :\n\n\\[({P}_k)\\left\\{\\begin{array}{l}\n  Min\\;\\;f_k(s)=\\frac{1}{2}\\|r(\\beta^{(k)})+J_r(\\beta^{(k)})s\\|^2\\\\\n  s\\in \\R^p,\n\\end{array}\\right.\\] where \\(s = \\beta - \\beta^{(k)}\\) abd \\(J_r(\\beta)\\) is the Jacobian matrix of \\(r\\) in \\(\\beta\\).\n\nData, Residuals and \\(f\\) functions\n\n\nCode\nusing LinearAlgebra\nusing Plots\nTi = [ 500, 1000, 2000, 3000, 4000, 5000, 6300];\nAi = [14.5, 13.5, 12.0, 10.8,  9.9,  8.9,  8.0];\nn = length(Ti)\nData_C14 = [Ti Ai];\nprintln(Data_C14)\n\n# Initialisation\nbeta0 = [10; 0.0001];   # Newton, Gauus-Newton, fminunc et leastsq converge\n# beta0 = [15; 0.001];    # Newton, Gauss-Newton, fminunc et leastsq divergent\n# beta0 = [15; 0.0005];   # Newton diverge, Gauss-Newton, fminunc et leastsq convergent\n# beta0 = [10; 0.0005];   # Gauss-Newton converge\n\n# Initial model\n#----------------------------------\nxmin = 9; xmax = 20;\nxx = range(xmin, stop=xmax, length=100);\n\nymin = -0.0001; ymax = 0.0005;\nyy = range(ymin, stop=ymax, length=100);\n\n# Residual function\nfunction r(β,data)\n    A₀ = β[1]\n    λ = β[2]\n    return data[:,2]-A₀*exp.(-λ*data[:,1])\nend\n# Plot of the function\n\nprintln(\"r(beta0,Data_C14) = \", r(beta0,Data_C14))\nX = [ones(n) Ti]\nf(β) = 0.5*norm(r(β,Data_C14))^2\nf_contour(A₀,λ) = f([A₀,λ])\nz = @. f_contour(xx', yy)\n\np1 = plot()\ncontour!(p1,xx,yy,z,levels=100,cbar=false,color=:turbo)\n\np2 = scatter(Ti,Ai,title=\"Data C14\")\nT = range(0,stop=6500,length=100);\nA = beta0[1]*exp.(-beta0[2]*T);\nplot!(p2,T,A)\n\n\n[500.0 14.5; 1000.0 13.5; 2000.0 12.0; 3000.0 10.8; 4000.0 9.9; 5000.0 8.9; 6300.0 8.0]\nr(beta0,Data_C14) = [4.987705754992859, 4.451625819640405, 3.812692469220181, 3.391817793182822, 3.1967995396436066, 2.8346934028736666, 2.674081989931028]\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith the Gauss-Newton Algorithm\n\n\nCode\n# solve by Gauss-Newton Algorithm\nusing Plots\ninclude(\"assets/julia/MyOptims.jl\")\nnb_levels = 5\nres(β) = r(β,Data_C14)\nfor nbit in 1:nb_levels\n    βsol, flag, fsol, ∇f_xsol , nb_iter  = algo_Gauss_Newton(res,beta0,nbit_max=nbit)\n #   A = βsol[1]*exp.(-βsol[2]*T);\n    plot!(p2,T,βsol[1]*exp.(-βsol[2]*T))\n    scatter!(p1,[βsol[1]],[βsol[2]])\nend\n\nplot(p1,p2,legend=false)\n\n\n┌ Error: Error during loading of extension SpecialFunctionsExt of ColorVectorSpace, use `Base.retry_load_extensions()` to retry.\n│   exception =\n│    1-element ExceptionStack:\n│    ArgumentError: Package SpecialFunctionsExt [997ecda8-951a-5f50-90ea-61382e97704b] is required but does not seem to be installed:\n│     - Run `Pkg.instantiate()` to install all recorded dependencies.\n│    \n│    Stacktrace:\n│      [1] _require(pkg::Base.PkgId, env::Nothing)\n│        @ Base ./loading.jl:2438\n│      [2] __require_prelocked(uuidkey::Base.PkgId, env::Nothing)\n│        @ Base ./loading.jl:2315\n│      [3] #invoke_in_world#3\n│        @ ./essentials.jl:1089 [inlined]\n│      [4] invoke_in_world\n│        @ ./essentials.jl:1086 [inlined]\n│      [5] _require_prelocked\n│        @ ./loading.jl:2302 [inlined]\n│      [6] _require_prelocked\n│        @ ./loading.jl:2301 [inlined]\n│      [7] run_extension_callbacks(extid::Base.ExtensionId)\n│        @ Base ./loading.jl:1500\n│      [8] run_extension_callbacks(pkgid::Base.PkgId)\n│        @ Base ./loading.jl:1535\n│      [9] run_package_callbacks(modkey::Base.PkgId)\n│        @ Base ./loading.jl:1354\n│     [10] _require_search_from_serialized(pkg::Base.PkgId, sourcepath::String, build_id::UInt128, stalecheck::Bool; reasons::Dict{String, Int64}, DEPOT_PATH::Vector{String})\n│        @ Base ./loading.jl:1993\n│     [11] _require(pkg::Base.PkgId, env::String)\n│        @ Base ./loading.jl:2450\n│     [12] __require_prelocked(uuidkey::Base.PkgId, env::String)\n│        @ Base ./loading.jl:2315\n│     [13] #invoke_in_world#3\n│        @ ./essentials.jl:1089 [inlined]\n│     [14] invoke_in_world\n│        @ ./essentials.jl:1086 [inlined]\n│     [15] _require_prelocked(uuidkey::Base.PkgId, env::String)\n│        @ Base ./loading.jl:2302\n│     [16] macro expansion\n│        @ ./loading.jl:2241 [inlined]\n│     [17] macro expansion\n│        @ ./lock.jl:273 [inlined]\n│     [18] __require(into::Module, mod::Symbol)\n│        @ Base ./loading.jl:2198\n│     [19] #invoke_in_world#3\n│        @ ./essentials.jl:1089 [inlined]\n│     [20] invoke_in_world\n│        @ ./essentials.jl:1086 [inlined]\n│     [21] require(into::Module, mod::Symbol)\n│        @ Base ./loading.jl:2191\n│     [22] include(fname::String)\n│        @ Main ./sysimg.jl:38\n│     [23] top-level scope\n│        @ In[4]:3\n│     [24] eval\n│        @ ./boot.jl:430 [inlined]\n│     [25] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n│        @ Base ./loading.jl:2643\n│     [26] softscope_include_string(m::Module, code::String, filename::String)\n│        @ SoftGlobalScope ~/.julia/packages/SoftGlobalScope/u4UzH/src/SoftGlobalScope.jl:65\n│     [27] execute_request(socket::ZMQ.Socket, msg::IJulia.Msg)\n│        @ IJulia ~/.julia/packages/IJulia/dR0lE/src/execute_request.jl:74\n│     [28] #invokelatest#2\n│        @ ./essentials.jl:1055 [inlined]\n│     [29] invokelatest\n│        @ ./essentials.jl:1052 [inlined]\n│     [30] eventloop(socket::ZMQ.Socket)\n│        @ IJulia ~/.julia/packages/IJulia/dR0lE/src/eventloop.jl:8\n│     [31] (::IJulia.var\"#15#18\")()\n│        @ IJulia ~/.julia/packages/IJulia/dR0lE/src/eventloop.jl:38\n└ @ Base loading.jl:1506\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith Newton Algorithm\n\n\nCode\n# solve by Newton Algorithm\n\nnb_levels = 5\nfor nbit in 1:nb_levels\n    βsol, flag, fsol, ∇f_xsol , nb_iter  = algo_Newton(f,beta0,nbit_max=nbit)\n #   A = βsol[1]*exp.(-βsol[2]*T);\n    plot!(p2,T,βsol[1]*exp.(-βsol[2]*T))\n    scatter!(p1,[βsol[1]],[βsol[2]])\nend\n\nplot(p1,p2,legend=false)",
    "crumbs": [
      "Least Squares Probem"
    ]
  }
]